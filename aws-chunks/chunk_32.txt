Determine the best model for your use case â Evaluate outputs of different models with built-in or custom prompt datasets to determine the model that is best suited for your application. Prevent inappropriate or unwanted content â Use guardrails to implement safeguards for your generative AI applications. Optimize your FM's latency â Get faster response times and improved responsiveness for AI applications with Latency-optimized inference for foundation models. Note
The Latency Optimized Inference feature is in preview release for Amazon Bedrock and is subject to change. To learn about Regions that support Amazon Bedrock and the foundation models and features that Amazon Bedrock supports, see Supported foundation models in Amazon Bedrock and Feature support by AWS Region in Amazon Bedrock
.