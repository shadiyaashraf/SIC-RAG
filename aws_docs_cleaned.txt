============================== What is Amazon Textract?
Amazon Textract helps you add document text detection and analysis to your applications. Using Amazon Textract, you can do the following: Detect typed and handwritten text in a variety of documents, including financial reports, medical records, and tax forms. Extract text, forms, and tables from documents with structured data, using the Amazon Textract Document Analysis API. Specify and extract information from documents using the Queries feature within the Amazon Textract Analyze Document API. Process invoices and receipts with the AnalyzeExpense API. Process ID documents such as drivers licenses and passports issued by U.S. government, using the AnalyzeID API. Upload and process mortgage loan packages, through automatic routing of the the document pages to the appropriate Amazon Textract analysis operations using the Analyze Lending workflow. You can retrieve analysis results for each document page or you can retrieve summarized results for a set of document pages. Use Custom Queries to customize the pretrained Queries feature using your data to support your down stream processing needs. Amazon Textract is based on the same proven, highly scalable, deep-learning technology that was developed by Amazon's computer vision scientists to analyze billions of images and videos daily. You don't need any machine learning expertise to use it, as Amazon Textract includes simple, easy-to-use API operations that can analyze image files and PDF files. Amazon Textract is always learning from new data, and Amazon is continually adding new features to the service.
The following are common use cases for using Amazon Textract: Creating an intelligent search index â Using Amazon Textract you can create libraries of text that is detected in image and PDF files. Using intelligent text extraction for natural language processing (NLP) â Amazon Textract provides you with control over how text is grouped as an input for NLP applications. It can extract text as words and lines. It also groups text by table cells if Amazon Textract document table analysis is enabled. Accelerating the capture and normalization of data from different sources â Amazon Textract enables text and tabular data extraction from a wide variety of documents, such as financial documents, research reports, and medical notes. With Amazon Textract Analyze Document APIs, you can easily and quickly extract unstructured and structured data from your documents. Automating data capture from forms â Amazon Textract enables structured data to be extracted from forms. With Amazon Textract Analysis APIs, you can build extraction capabilities into existing business workflows so that user data submitted through forms can be extracted into a usable format. Automating document classification and extraction â With Amazon Textract's Analyze Lending document processing API, you can automate the classification of lending documents into various document classes, and then automatically route the classified pages to the correct analysis operation for further processing. Some of the benefits of using Amazon Textract include: Integration of document text detection into your apps â Amazon Textract removes the complexity of building text detection capabilities into your applications by making powerful and accurate analysis available with a simple API. You donât need computer vision or deep learning expertise to use Amazon Textract to detect document text. With Amazon Textract Text APIs, you can easily build text detection into any web, mobile, or connected device application. Scalable document analysis â Amazon Textract enables you to analyze and extract data quickly from millions of documents, which can accelerate decision making. Low cost â With Amazon Textract, you only pay for the documents you analyze. There are no minimum fees or upfront commitments. You can get started for free, and save more as you grow with our tiered pricing model. With synchronous processing, Amazon Textract can analyze single-page documents for applications where latency is critical. Amazon Textract also provides asynchronous operations to extend support to multipage documents. Amazon Textract's API operations have quotas that limit how quickly and how often you can use them. If the limit set for your account is frequently exceeded, you can request a limit increase. To change a limit, select the Amazon Textract option in the Service Quotas console. You can use the Quotas Calculator in the Amazon Textract console to determine your quota requirements. To learn more about default quotas that can be changed, see Information on Default Quotas in Amazon Textract
.
Other quotas, like file size and languages supported by Amazon Textract, cannot be changed. For more information on set quotas, see Set Quotas in Amazon Textract
. First-Time Amazon Textract Users If this is your first time using Amazon Textract, we recommend that you read the following sections in order: Identifying Your Amazon Textract Use Case â This section introduces the Amazon Textract components and how they work together for an end-to-end experience. Getting Started with Amazon Textract â In this section, you set up your account and test the Amazon Textract API. Javascript is disabled or is unavailable in your browser.
To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.
Document Conventions
Getting Started ============================== Identifying Your Amazon Textract Use Case
Amazon Textract offers a variety of operations that apply to different documents. Below is a list of the operations you can perform with Amazon Textract and links to further information on each use case. Detecting text only. For more information, see Detecting Text
. Detecting and analyzing relationships between text. For more information, see Analyzing Documents
. Detecting and analyzing text in invoices and receipts. For more information, see Analyzing Invoices and Receipts
. Detecting and analyzing text in government identity documents. For more information, see Analyzing Identity Documents
. Detecting and analyzing text in lending documents. For more information, see Analyzing Lending Documents
. Amazon Textract provides you with synchronous operations for processing single-page documents with near real-time responses. For more information, see Processing Documents Synchronously
. Amazon Textract also provides asynchronous operations that you can use to process larger, multipage documents. Asynchronous responses aren't in real time. For more information, see Processing Documents Asynchronously
. Amazon Textract provides you with a workflow to automatically classify lending document pages and route them to existing solutions. For more information see Analyzing Lending Documents
. Amazon Textract lets you customize the output of its pretrained Queries feature. With Amazon Textract Custom Queries, you can use your own documents and train an adapter to customize the base model, keeping complete control over your proprietary documents. See Customizing your Queries Responses for more information. For information regarding the results returned by Analyze Lending, see Analyze Lending Response Objects .
Topics
Detecting Text
Analyzing Documents
Analyzing Invoices and Receipts
Analyzing Identity Documents
Analyzing Lending Documents
Customizing Outputs Javascript is disabled or is unavailable in your browser.
To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.
Document Conventions
Step 3: Get Started Using the AWS CLI and AWS SDK API
Detecting Text ============================== Quotas in Amazon Textract
The following sections provide information about quotas, formerly referred to as limits, when using Amazon Textract. There are two kinds of quotas. Set quotas
, which can be viewed in the section Set Quotas in Amazon Textract
, cannot be changed. Default quotas
, discussed in the section Default Quotas
, can be viewed or changed via the Service quotas console
. You can also view the current Amazon Textract default quotas on the Amazon Textract endpoints and service quotas
. Javascript is disabled or is unavailable in your browser.
To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.
Document Conventions
Warning
Set Quotas ============================== Actions
The following actions are supported: AnalyzeDocument AnalyzeExpense AnalyzeID CreateAdapter CreateAdapterVersion DeleteAdapter DeleteAdapterVersion DetectDocumentText GetAdapter GetAdapterVersion GetDocumentAnalysis GetDocumentTextDetection GetExpenseAnalysis GetLendingAnalysis GetLendingAnalysisSummary ListAdapters ListAdapterVersions ListTagsForResource StartDocumentAnalysis StartDocumentTextDetection StartExpenseAnalysis StartLendingAnalysis TagResource UntagResource UpdateAdapter Javascript is disabled or is unavailable in your browser.
To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.
Document Conventions
API Reference
AnalyzeDocument ============================== Amazon Textract ============================== What is Amazon Bedrock?
Amazon Bedrock is a fully managed service that makes high-performing foundation models (FMs) from leading AI companies and Amazon available for your use through a unified API. You can choose from a wide range of foundation models to find the model that is best suited for your use case. Amazon Bedrock also offers a broad set of capabilities to build generative AI applications with security, privacy, and responsible AI. Using Amazon Bedrock, you can easily experiment with and evaluate top foundation models for your use cases, privately customize them with your data using techniques such as fine-tuning and Retrieval Augmented Generation (RAG), and build agents that execute tasks using your enterprise systems and data sources.
With Amazon Bedrock's serverless experience, you can get started quickly, privately customize foundation models with your own data, and easily and securely integrate and deploy them into your applications using AWS tools without having to manage any infrastructure.
Topics
What can I do with Amazon Bedrock?
How do I get started with Amazon Bedrock?
Amazon Bedrock pricing
Key terminology What can I do with Amazon Bedrock? You can use Amazon Bedrock to do the following: Experiment with prompts and configurations â Submit prompts and generate responses with model inference by sending prompts using different configurations and foundation models to generate responses. You can use the API or the text, image, and chat playgrounds in the console to experiment in a graphical interface. When you're ready, set up your application to make requests to the InvokeModel APIs. Augment response generation with information from your data sources â Create knowledge bases by uploading data sources to be queried in order to augment a foundation model's generation of responses. Create applications that reason through how to help a customer â Build agents that use foundation models, make API calls, and (optionally) query knowledge bases in order to reason through and carry out tasks for your customers. Adapt models to specific tasks and domains with training data â Customize an Amazon Bedrock foundation model by providing training data for fine-tuning or continued-pretraining in order to adjust a model's parameters and improve its performance on specific tasks or in certain domains. Improve your FM-based application's efficiency and output â Purchase Provisioned Throughput for a foundation model in order to run inference on models more efficiently and at discounted rates. Determine the best model for your use case â Evaluate outputs of different models with built-in or custom prompt datasets to determine the model that is best suited for your application. Prevent inappropriate or unwanted content â Use guardrails to implement safeguards for your generative AI applications. Optimize your FM's latency â Get faster response times and improved responsiveness for AI applications with Latency-optimized inference for foundation models. Note
The Latency Optimized Inference feature is in preview release for Amazon Bedrock and is subject to change. To learn about Regions that support Amazon Bedrock and the foundation models and features that Amazon Bedrock supports, see Supported foundation models in Amazon Bedrock and Feature support by AWS Region in Amazon Bedrock
. How do I get started with Amazon Bedrock? We recommend that you start with Amazon Bedrock by doing the following: Familiarize yourself with the terms and concepts that Amazon Bedrock uses. Understand how AWS charges you for using Amazon Bedrock. Try the Get started with Amazon Bedrock tutorials. In the tutorials, you learn how to use the playgrounds in Amazon Bedrock console
. You also learn and how to use the AWS SDK to call Amazon Bedrock API operations. Read the documentation for the features that you want to include in your application. Javascript is disabled or is unavailable in your browser.
To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.
Document Conventions
Amazon Bedrock pricing ============================== Security in Amazon Bedrock
Cloud security at AWS is the highest priority. As an AWS customer, you benefit from data centers and network architectures that are built to meet the requirements of the most security-sensitive organizations.
Security is a shared responsibility between AWS and you. The shared responsibility model describes this as security of the cloud and security in the cloud: Security of the cloud â AWS is responsible for protecting the infrastructure that runs AWS services in the AWS Cloud. AWS also provides you with services that you can use securely. Third-party auditors regularly test and verify the effectiveness of our security as part of the AWS Compliance Programs
. To learn about the compliance programs that apply to Amazon Bedrock, see AWS Services in Scope by Compliance Program
. Security in the cloud â Your responsibility is determined by the AWS service that you use. You are also responsible for other factors including the sensitivity of your data, your companyâs requirements, and applicable laws and regulations. This documentation helps you understand how to apply the shared responsibility model when using Amazon Bedrock. The following topics show you how to configure Amazon Bedrock to meet your security and compliance objectives. You also learn how to use other AWS services that help you to monitor and secure your Amazon Bedrock resources. Topics
Data protection
Identity and access management for Amazon Bedrock
Cross-account access to Amazon S3 bucket for custom model import jobs
Compliance validation for Amazon Bedrock
Incident response in Amazon Bedrock
Resilience in Amazon Bedrock
Infrastructure security in Amazon Bedrock
Cross-service confused deputy prevention
Configuration and vulnerability analysis in Amazon Bedrock
Prompt injection security Javascript is disabled or is unavailable in your browser.
To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.
Document Conventions
Amazon Titan Image Generator G1 models overview
Data protection ============================== Amazon Bedrock ============================== Amazon Bedrock ============================== Amazon Bedrock ============================== What is Amazon S3?
Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Customers of all sizes and industries can use Amazon S3 to store and protect any amount of data for a range of use cases, such as data lakes, websites, mobile applications, backup and restore, archive, enterprise applications, IoT devices, and big data analytics. Amazon S3 provides management features so that you can optimize, organize, and configure access to your data to meet your specific business, organizational, and compliance requirements.
Note
For more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets
.
Topics
Features of Amazon S3
How Amazon S3 works
Amazon S3 data consistency model
Related services Accessing Amazon S3
Paying for Amazon S3
PCI DSS compliance Features of Amazon S3 Storage classes Amazon S3 offers a range of storage classes designed for different use cases. For example, you can store mission-critical production data in S3 Standard or S3 Express One Zone for frequent access, save costs by storing infrequently accessed data in S3 Standard-IA or S3 One Zone-IA, and archive data at the lowest costs in S3 Glacier Instant Retrieval, S3 Glacier Flexible Retrieval, and S3 Glacier Deep Archive. Amazon S3 Express One Zone is a high-performance, single-zone Amazon S3 storage class that is purpose-built to deliver consistent, single-digit millisecond data access for your most latency-sensitive applications. S3 Express One Zone is the lowest latency cloud object storage class available today, with data access speeds up to 10x faster and with request costs 50 percent lower than S3 Standard. S3 Express One Zone is the first S3 storage class where you can select a single Availability Zone with the option to co-locate your object storage with your compute resources, which provides the highest possible access speed. Additionally, to further increase access speed and support hundreds of thousands of requests per second, data is stored in a new bucket type: an Amazon S3 directory bucket. For more information, see S3 Express One Zone and Working with directory buckets
. You can store data with changing or unknown access patterns in S3 Intelligent-Tiering, which optimizes storage costs by automatically moving your data between four access tiers when your access patterns change. These four access tiers include two low-latency access tiers optimized for frequent and infrequent access, and two opt-in archive access tiers designed for asynchronous access for rarely accessed data. For more information, see Understanding and managing Amazon S3 storage classes
. Storage management Amazon S3 has storage management features that you can use to manage costs, meet regulatory requirements, reduce latency, and save multiple distinct copies of your data for compliance requirements. S3 Lifecycle â Configure a lifecycle configuration to manage your objects and store them cost effectively throughout their lifecycle. You can transition objects to other S3 storage classes or expire objects that reach the end of their lifetimes. S3 Object Lock â Prevent Amazon S3 objects from being deleted or overwritten for a fixed amount of time or indefinitely. You can use Object Lock to help meet regulatory requirements that require write-once-read-many (WORM) storage or to simply add another layer of protection against object changes and deletions. S3 Replication â Replicate objects and their respective metadata and object tags to one or more destination buckets in the same or different AWS Regions for reduced latency, compliance, security, and other use cases. S3 Batch Operations â Manage billions of objects at scale with a single S3 API request or a few clicks in the Amazon S3 console. You can use Batch Operations to perform operations such as Copy
, Invoke AWS Lambda function
, and Restore on millions or billions of objects. Access management and security Amazon S3 provides features for auditing and managing access to your buckets and objects. By default, S3 buckets and the objects in them are private. You have access only to the S3 resources that you create. To grant granular resource permissions that support your specific use case or to audit the permissions of your Amazon S3 resources, you can use the following features. S3 Block Public Access â Block public access to S3 buckets and objects. By default, Block Public Access settings are turned on at the bucket level. We recommend that you keep all Block Public Access settings enabled unless you know that you need to turn off one or more of them for your specific use case. For more information, see Configuring block public access settings for your S3 buckets
. AWS Identity and Access Management (IAM) â IAM is a web service that helps you securely control access to AWS resources, including your Amazon S3 resources. With IAM, you can centrally manage permissions that control which AWS resources users can access. You use IAM to control who is authenticated (signed in) and authorized (has permissions) to use resources. Bucket policies â Use IAM-based policy language to configure resource-based permissions for your S3 buckets and the objects in them. Amazon S3 access points â Configure named network endpoints with dedicated access policies to manage data access at scale for shared datasets in Amazon S3. Access control lists (ACLs) â Grant read and write permissions for individual buckets and objects to authorized users. As a general rule, we recommend using S3 resource-based policies (bucket policies and access point policies) or IAM user policies for access control instead of ACLs. Policies are a simplified and more flexible access control option. With bucket policies and access point policies, you can define rules that apply broadly across all requests to your Amazon S3 resources. For more information about the specific cases when you'd use ACLs instead of resource-based policies or IAM user policies, see Managing access with ACLs
. S3 Object Ownership â Take ownership of every object in your bucket, simplifying access management for data stored in Amazon S3. S3 Object Ownership is an Amazon S3 bucket-level setting that you can use to disable or enable ACLs. By default, ACLs are disabled. With ACLs disabled, the bucket owner owns all the objects in the bucket and manages access to data exclusively by using access-management policies. IAM Access Analyzer for S3 â Evaluate and monitor your S3 bucket access policies, ensuring that the policies provide only the intended access to your S3 resources. Data processing To transform data and trigger workflows to automate a variety of other processing activities at scale, you can use the following features. S3 Object Lambda â Add your own code to S3 GET, HEAD, and LIST requests to modify and process data as it is returned to an application. Filter rows, dynamically resize images, redact confidential data, and much more. Event notifications â Trigger workflows that use Amazon Simple Notification Service (Amazon SNS), Amazon Simple Queue Service (Amazon SQS), and AWS Lambda when a change is made to your S3 resources. Storage logging and monitoring Amazon S3 provides logging and monitoring tools that you can use to monitor and control how your Amazon S3 resources are being used. For more information, see Monitoring tools
. Automated monitoring tools Amazon CloudWatch metrics for Amazon S3 â Track the operational health of your S3 resources and configure billing alerts when estimated charges reach a user-defined threshold. AWS CloudTrail â Record actions taken by a user, a role, or an AWS service in Amazon S3. CloudTrail logs provide you with detailed API tracking for S3 bucket-level and object-level operations. Manual monitoring tools Server access logging â Get detailed records for the requests that are made to a bucket. You can use server access logs for many use cases, such as conducting security and access audits, learning about your customer base, and understanding your Amazon S3 bill. AWS Trusted Advisor â Evaluate your account by using AWS best practice checks to identify ways to optimize your AWS infrastructure, improve security and performance, reduce costs, and monitor service quotas. You can then follow the recommendations to optimize your services and resources. Analytics and insights Amazon S3 offers features to help you gain visibility into your storage usage, which empowers you to better understand, analyze, and optimize your storage at scale. Amazon S3 Storage Lens â Understand, analyze, and optimize your storage. S3 Storage Lens provides 60+ usage and activity metrics and interactive dashboards to aggregate data for your entire organization, specific accounts, AWS Regions, buckets, or prefixes. Storage Class Analysis â Analyze storage access patterns to decide when it's time to move data to a more cost-effective storage class. S3 Inventory with Inventory reports â Audit and report on objects and their corresponding metadata and configure other Amazon S3 features to take action in Inventory reports. For example, you can report on the replication and encryption status of your objects. For a list of all the metadata available for each object in Inventory reports, see Amazon S3 Inventory list
. Strong consistency Amazon S3 provides strong read-after-write consistency for PUT and DELETE requests of objects in your Amazon S3 bucket in all AWS Regions. This behavior applies to both writes of new objects as well as PUT requests that overwrite existing objects and DELETE requests. In addition, read operations on Amazon S3 Select, Amazon S3 access control lists (ACLs), Amazon S3 Object Tags, and object metadata (for example, the HEAD object) are strongly consistent. For more information, see Amazon S3 data consistency model
. How Amazon S3 works Amazon S3 is an object storage service that stores data as objects, hierarchical data, or tabular data within buckets. An object is a file and any metadata that describes the file. A bucket is a container for objects. To store your data in Amazon S3, you first create a bucket and specify a bucket name and AWS Region. Then, you upload your data to that bucket as objects in Amazon S3. Each object has a key (or key name
), which is the unique identifier for the object within the bucket. S3 provides features that you can configure to support your specific use case. For example, you can use S3 Versioning to keep multiple versions of an object in the same bucket, which allows you to restore objects that are accidentally deleted or overwritten. Buckets and the objects in them are private and can be accessed only if you explicitly grant access permissions. You can use bucket policies, AWS Identity and Access Management (IAM) policies, access control lists (ACLs), and S3 Access Points to manage access. Topics
Buckets
Objects
Keys
S3 Versioning
Version ID
Bucket policy
S3 access points
Access control lists (ACLs)
Regions Buckets Amazon S3 supports four types of bucketsâgeneral purpose buckets, directory buckets, table buckets, and vector buckets. Each type of bucket provides a unique set of features for different use cases. General purpose buckets â General purpose buckets are recommended for most use cases and access patterns and are the original S3 bucket type. A general purpose bucket is a container for objects stored in Amazon S3, and you can store any number of objects in a bucket and across all storage classes (except for S3 Express One Zone), so you can redundantly store objects across multiple Availability Zones. For more information, see Creating, configuring, and working with Amazon S3 general purpose buckets
. Note
By default, all general purpose buckets are private. However, you can grant public access to general purpose buckets. You can control access to general purpose buckets at the bucket, prefix (folder), or object tag level. For more information, see Access control in Amazon S3
. Directory buckets â Recommended for low-latency use cases and data-residency use cases. By default, you can create up to 100 directory buckets in your AWS account, with no limit on the number of objects that you can store in a directory bucket. Directory buckets organize objects into hierarchical directories (prefixes) instead of the flat storage structure of general purpose buckets. This bucket type has no prefix limits and individual directories can scale horizontally. For more information, see Working with directory buckets
. For low-latency use cases, you can create a directory bucket in a single AWS Availability Zone to store data. Directory buckets in Availability Zones support the S3 Express One Zone storage class. With S3 Express One Zone, your data is redundantly stored on multiple devices within a single Availability Zone. The S3 Express One Zone storage class is recommended if your application is performance sensitive and benefits from single-digit millisecond PUT and GET latencies. To learn more about creating directory buckets in Availability Zones, see High performance workloads
. For data-residency use cases, you can create a directory bucket in a single AWS Dedicated Local Zone (DLZ) to store data. In Dedicated Local Zones, you can create S3 directory buckets to store data in a specific data perimeter, which helps support your data residency and isolation use cases. Directory buckets in Local Zones support the S3 One Zone-Infrequent Access (S3 One Zone-IA; Z-IA) storage class. To learn more about creating directory buckets in Local Zones, see Data residency workloads
. Note
Directory buckets have all public access disabled by default. This behavior can't be changed. You can't grant access to objects stored in directory buckets. You can grant access only to your directory buckets. For more information, see Authenticating and authorizing requests
. Table buckets â Recommended for storing tabular data, such as daily purchase transactions, streaming sensor data, or ad impressions. Tabular data represents data in columns and rows, like in a database table. Table buckets provide S3 storage that's optimized for analytics and machine learning workloads, with features designed to continuously improve query performance and reduce storage costs for tables. S3 Tables are purpose-built for storing tabular data in the Apache Iceberg format. You can query tabular data in S3 Tables with popular query engines, including Amazon Athena, Amazon Redshift, and Apache Spark. By default, you can create up to 10 table buckets per AWS account per AWS Region and up to 10,000 tables per table bucket. For more information, see Working with S3 Tables and table buckets
. Note
All table buckets and tables are private and can't be made public. These resources can only be accessed by users who are explicitly granted access. To grant access, you can use IAM resource-based policies for table buckets and tables, and IAM identity-based policies for users and roles. For more information, see Security for S3 Tables
. Vector buckets â S3 vector buckets are a type of Amazon S3 bucket that are purpose-built to store and query vectors. Vector buckets use dedicated API operations to write and query vector data efficiently. With S3 vector buckets, you can store vector embeddings for machine learning models, perform similarity searches, and integrate with services like Amazon Bedrock and Amazon OpenSearch. S3 vector buckets organize data using vector indexes, which are resources within a bucket that store and organize vector data for efficient similarity search. Each vector index can be configured with specific dimensions, distance metrics (like cosine similarity), and metadata configurations to optimize for your specific use case. For more information, see Working with S3 Vectors and vector buckets
. Additional information about all bucket types When you create a bucket, you enter a bucket name and choose the AWS Region where the bucket will reside. After you create a bucket, you cannot change the name of the bucket or its Region. Bucket names must follow the following bucket naming rules: General purpose bucket naming rules
Directory bucket naming rules
Table bucket naming rules Buckets also: Organize the Amazon S3 namespace at the highest level. For general purpose buckets, this namespace is S3
. For directory buckets, this namespace is s3express
. For table buckets, this namespace is s3tables
. Identify the account responsible for storage and data transfer charges. Serve as the unit of aggregation for usage reporting. Objects Objects are the fundamental entities stored in Amazon S3. Objects consist of object data and metadata. The metadata is a set of name-value pairs that describe the object. These pairs include some default metadata, such as the date last modified, and standard HTTP metadata, such as Content-Type
. You can also specify custom metadata at the time that the object is stored. Every object is contained in a bucket. For example, if the object named photos/puppy.jpg is stored in the amzn-s3-demo-bucket general purpose bucket in the US West (Oregon) Region, then it is addressable by using the URL https://amzn-s3-demo-bucket.s3.us-west-2.amazonaws.com/photos/puppy.jpg
. For more information, see Accessing a Bucket
. An object is uniquely identified within a bucket by a key (name) and a version ID (if S3 Versioning is enabled on the bucket). For more information about objects, see Amazon S3 objects overview
. Keys An object key (or key name
) is the unique identifier for an object within a bucket. Every object in a bucket has exactly one key. The combination of a bucket, object key, and optionally, version ID (if S3 Versioning is enabled for the bucket) uniquely identify each object. So you can think of Amazon S3 as a basic data map between "bucket + key + version" and the object itself. Every object in Amazon S3 can be uniquely addressed through the combination of the web service endpoint, bucket name, key, and optionally, a version. For example, in the URL https://
amzn-s3-demo-bucket
.s3.us-west-2.amazonaws.com/photos/puppy.jpg
, amzn-s3-demo-bucket is the name of the bucket and photos/puppy.jpg is the key. For more information about object keys, see Naming Amazon S3 objects
. S3 Versioning You can use S3 Versioning to keep multiple variants of an object in the same bucket. With S3 Versioning, you can preserve, retrieve, and restore every version of every object stored in your buckets. You can easily recover from both unintended user actions and application failures. For more information, see Retaining multiple versions of objects with S3 Versioning
. Version ID When you enable S3 Versioning in a bucket, Amazon S3 generates a unique version ID for each object added to the bucket. Objects that already existed in the bucket at the time that you enable versioning have a version ID of null
. If you modify these (or any other) objects with other operations, such as CopyObject and PutObject
, the new objects get a unique version ID. For more information, see Retaining multiple versions of objects with S3 Versioning
. Bucket policy A bucket policy is a resource-based AWS Identity and Access Management (IAM) policy that you can use to grant access permissions to your bucket and the objects in it. Only the bucket owner can associate a policy with a bucket. The permissions attached to the bucket apply to all of the objects in the bucket that are owned by the bucket owner. Bucket policies are limited to 20 KB in size. Bucket policies use JSON-based access policy language that is standard across AWS. You can use bucket policies to add or deny permissions for the objects in a bucket. Bucket policies allow or deny requests based on the elements in the policy, including the requester, S3 actions, resources, and aspects or conditions of the request (for example, the IP address used to make the request). For example, you can create a bucket policy that grants cross-account permissions to upload objects to an S3 bucket while ensuring that the bucket owner has full control of the uploaded objects. For more information, see Examples of Amazon S3 bucket policies
. In your bucket policy, you can use wildcard characters on Amazon Resource Names (ARNs) and other values to grant permissions to a subset of objects. For example, you can control access to groups of objects that begin with a common prefix or end with a given extension, such as .html
. S3 access points Amazon S3 access points are named network endpoints with dedicated access policies that describe how data can be accessed using that endpoint. Access points are attached to an underlying data source, such as a general purpose bucket, directory bucket, or a FSx for OpenZFS volume, that you can use to perform S3 object operations, such as GetObject and PutObject
. Access points simplify managing data access at scale for shared datasets in Amazon S3. Each access point has its own access point policy. You can configure Block Public Access settings for each access point attached to a bucket. To restrict Amazon S3 data access to a private network, you can also configure any access point to accept requests only from a virtual private cloud (VPC). For more information about access points for general purpose buckets, see Managing access to shared datasets with access points
. For more information about access points for directory buckets, see Managing access to shared datasets in directory buckets with access points
. Access control lists (ACLs) You can use ACLs to grant read and write permissions to authorized users for individual general purpose buckets and objects. Each general purpose bucket and object has an ACL attached to it as a subresource. The ACL defines which AWS accounts or groups are granted access and the type of access. ACLs are an access control mechanism that predates IAM. For more information about ACLs, see Access control list (ACL) overview
. S3 Object Ownership is an Amazon S3 bucket-level setting that you can use to both control ownership of the objects that are uploaded to your bucket and to disable or enable ACLs. By default, Object Ownership is set to the Bucket owner enforced setting, and all ACLs are disabled. When ACLs are disabled, the bucket owner owns all the objects in the bucket and manages access to them exclusively by using access-management policies. A majority of modern use cases in Amazon S3 no longer require the use of ACLs. We recommend that you keep ACLs disabled, except in circumstances where you need to control access for each object individually. With ACLs disabled, you can use policies to control access to all objects in your bucket, regardless of who uploaded the objects to your bucket. For more information, see Controlling ownership of objects and disabling ACLs for your bucket
. Regions You can choose the geographical AWS Region where Amazon S3 stores the buckets that you create. You might choose a Region to optimize latency, minimize costs, or address regulatory requirements. Objects stored in an AWS Region never leave the Region unless you explicitly transfer or replicate them to another Region. For example, objects stored in the Europe (Ireland) Region never leave it. Note
You can access Amazon S3 and its features only in the AWS Regions that are enabled for your account. For more information about enabling a Region to create and manage AWS resources, see Managing AWS Regions in the AWS General Reference
. For a list of Amazon S3 Regions and endpoints, see Regions and endpoints in the AWS General Reference
. Amazon S3 data consistency model Amazon S3 provides strong read-after-write consistency for PUT and DELETE requests of objects in your Amazon S3 bucket in all AWS Regions. This behavior applies to both writes to new objects as well as PUT requests that overwrite existing objects and DELETE requests. In addition, read operations on Amazon S3 Select, Amazon S3 access controls lists (ACLs), Amazon S3 Object Tags, and object metadata (for example, the HEAD object) are strongly consistent. Updates to a single key are atomic. For example, if you make a PUT request to an existing key from one thread and perform a GET request on the same key from a second thread concurrently, you will get either the old data or the new data, but never partial or corrupt data. Amazon S3 achieves high availability by replicating data across multiple servers within AWS data centers. If a PUT request is successful, your data is safely stored. Any read (GET or LIST request) that is initiated following the receipt of a successful PUT response will return the data written by the PUT request. Here are examples of this behavior: A process writes a new object to Amazon S3 and immediately lists keys within its bucket. The new object appears in the list. A process replaces an existing object and immediately tries to read it. Amazon S3 returns the new data. A process deletes an existing object and immediately tries to read it. Amazon S3 does not return any data because the object has been deleted. A process deletes an existing object and immediately lists keys within its bucket. The object does not appear in the listing. Note Amazon S3 does not support object locking for concurrent writers. If two PUT requests are simultaneously made to the same key, the request with the latest timestamp wins. If this is an issue, you must build an object-locking mechanism into your application. Updates are key-based. There is no way to make atomic updates across keys. For example, you cannot make the update of one key dependent on the update of another key unless you design this functionality into your application. Bucket configurations have an eventual consistency model. Specifically, this means that: If you delete a bucket and immediately list all buckets, the deleted bucket might still appear in the list. If you enable versioning on a bucket for the first time, it might take a short amount of time for the change to be fully propagated. We recommend that you wait for 15 minutes after enabling versioning before issuing write operations (PUT or DELETE requests) on objects in the bucket. Concurrent applications This section provides examples of behavior to be expected from Amazon S3 when multiple clients are writing to the same items. In this example, both W1 (write 1) and W2 (write 2) finish before the start of R1 (read 1) and R2 (read 2). Because S3 is strongly consistent, R1 and R2 both return color = ruby
. In the next example, W2 does not finish before the start of R1. Therefore, R1 might return color = ruby or color = garnet
. However, because W1 and W2 finish before the start of R2, R2 returns color = garnet
. In the last example, W2 begins before W1 has received an acknowledgment. Therefore, these writes are considered concurrent. Amazon S3 internally uses last-writer-wins semantics to determine which write takes precedence. However, the order in which Amazon S3 receives the requests and the order in which applications receive acknowledgments cannot be predicted because of various factors, such as network latency. For example, W2 might be initiated by an Amazon EC2 instance in the same Region, while W1 might be initiated by a host that is farther away. The best way to determine the final value is to perform a read after both writes have been acknowledged. Related services After you load your data into Amazon S3, you can use it with other AWS services. The following are the services that you might use most frequently: Amazon Elastic Compute Cloud (Amazon EC2) â Provides secure and scalable computing capacity in the AWS Cloud. Using Amazon EC2 eliminates your need to invest in hardware upfront, so you can develop and deploy applications faster. You can use Amazon EC2 to launch as many or as few virtual servers as you need, configure security and networking, and manage storage. Amazon EMR â Helps businesses, researchers, data analysts, and developers easily and cost-effectively process vast amounts of data. Amazon EMR uses a hosted Hadoop framework running on the web-scale infrastructure of Amazon EC2 and Amazon S3. AWS Snow Family â Helps customers that need to run operations in austere, non-data center environments, and in locations where there's a lack of consistent network connectivity. You can use AWS Snow Family devices to locally and cost-effectively access the storage and compute power of the AWS Cloud in places where an internet connection might not be an option. AWS Transfer Family â Provides fully managed support for file transfers directly into and out of Amazon S3 or Amazon Elastic File System (Amazon EFS) using Secure Shell (SSH) File Transfer Protocol (SFTP), File Transfer Protocol over SSL (FTPS), and File Transfer Protocol (FTP). Accessing Amazon S3 You can work with Amazon S3 in any of the following ways: AWS Management Console The console is a web-based user interface for managing Amazon S3 and AWS resources. If you've signed up for an AWS account, you can access the Amazon S3 console by signing into the AWS Management Console and choosing S3 from the AWS Management Console home page. AWS Command Line Interface You can use the AWS command line tools to issue commands or build scripts at your system's command line to perform AWS (including S3) tasks. The AWS Command Line Interface (AWS CLI) provides commands for a broad set of AWS services. The AWS CLI is supported on Windows, macOS, and Linux. To get started, see the AWS Command Line Interface User Guide
. For more information about the commands for Amazon S3, see s3api and s3control in the AWS CLI Command Reference
. AWS SDKs AWS provides SDKs (software development kits) that consist of libraries and sample code for various programming languages and platforms (Java, Python, Ruby, .NET, iOS, Android, and so on). The AWS SDKs provide a convenient way to create programmatic access to S3 and AWS. Amazon S3 is a REST service. You can send requests to Amazon S3 using the AWS SDK libraries, which wrap the underlying Amazon S3 REST API and simplify your programming tasks. For example, the SDKs take care of tasks such as calculating signatures, cryptographically signing requests, managing errors, and retrying requests automatically. For information about the AWS SDKs, including how to download and install them, see Tools for AWS
. Every interaction with Amazon S3 is either authenticated or anonymous. If you are using the AWS SDKs, the libraries compute the signature for authentication from the keys that you provide. For more information about how to make requests to Amazon S3, see Making requests . Amazon S3 REST API The architecture of Amazon S3 is designed to be programming language-neutral, using AWS-supported interfaces to store and retrieve objects. You can access S3 and AWS programmatically by using the Amazon S3 REST API. The REST API is an HTTP interface to Amazon S3. With the REST API, you use standard HTTP requests to create, fetch, and delete buckets and objects. To use the REST API, you can use any toolkit that supports HTTP. You can even use a browser to fetch objects, as long as they are anonymously readable. The REST API uses standard HTTP headers and status codes, so that standard browsers and toolkits work as expected. In some areas, we have added functionality to HTTP (for example, we added headers to support access control). In these cases, we have done our best to add the new functionality in a way that matches the style of standard HTTP usage. If you make direct REST API calls in your application, you must write the code to compute the signature and add it to the request. For more information about how to make requests to Amazon S3, see Making requests in the Amazon S3 API Reference
. Note
SOAP API support over HTTP is deprecated, but it is still available over HTTPS. Newer Amazon S3 features are not supported for SOAP. We recommend that you use either the REST API or the AWS SDKs. Paying for Amazon S3 Pricing for Amazon S3 is designed so that you don't have to plan for the storage requirements of your application. Most storage providers require you to purchase a predetermined amount of storage and network transfer capacity. In this scenario, if you exceed that capacity, your service is shut off or you are charged high overage fees. If you do not exceed that capacity, you pay as though you used it all. Amazon S3 charges you only for what you actually use, with no hidden fees and no overage charges. This model gives you a variable-cost service that can grow with your business while giving you the cost advantages of the AWS infrastructure. For more information, see Amazon S3 Pricing
. When you sign up for AWS, your AWS account is automatically signed up for all services in AWS, including Amazon S3. However, you are charged only for the services that you use. If you are a new Amazon S3 customer, you can get started with Amazon S3 for free. For more information, see AWS free tier
. To see your bill, go to the Billing and Cost Management Dashboard in the AWS Billing and Cost Management console
. To learn more about AWS account billing, see the AWS Billing User Guide
. If you have questions concerning AWS billing and AWS accounts, contact AWS Support
. PCI DSS compliance Amazon S3 supports the processing, storage, and transmission of credit card data by a merchant or service provider, and has been validated as being compliant with Payment Card Industry (PCI) Data Security Standard (DSS). For more information about PCI DSS, including how to request a copy of the AWS PCI Compliance Package, see PCI DSS Level 1
. Javascript is disabled or is unavailable in your browser.
To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.
Document Conventions
Amazon S3 Object Lambda availability change ============================== How Amazon S3 works with IAM
Before you use IAM to manage access to Amazon S3, learn what IAM features are available to use with Amazon S3.
IAM features you can use with Amazon S3 IAM feature Amazon S3 support Identity-based policies Yes Resource-based policies Yes Policy actions Yes Policy resources Yes Policy condition keys (service-specific) Yes ACLs Yes ABAC (tags in policies) Partial Temporary credentials Yes Forward access sessions (FAS) Yes Service roles Yes Service-linked roles Partial To get a high-level view of how Amazon S3 and other AWS services work with most IAM features, see AWS services that work with IAM in the IAM User Guide
.
For more information about the permissions to S3 API operations by S3 resource types, see Required permissions for Amazon S3 API operations
. Identity-based policies for Amazon S3 Supports identity-based policies: Yes Identity-based policies are JSON permissions policy documents that you can attach to an identity, such as an IAM user, group of users, or role. These policies control what actions users and roles can perform, on which resources, and under what conditions. To learn how to create an identity-based policy, see Define custom IAM permissions with customer managed policies in the IAM User Guide
. With IAM identity-based policies, you can specify allowed or denied actions and resources as well as the conditions under which actions are allowed or denied. To learn about all of the elements that you can use in a JSON policy, see IAM JSON policy elements reference in the IAM User Guide
. Identity-based policy examples for Amazon S3 To view examples of Amazon S3 identity-based policies, see Identity-based policies for Amazon S3
. Resource-based policies within Amazon S3 Supports resource-based policies: Yes Resource-based policies are JSON policy documents that you attach to a resource. Examples of resource-based policies are IAM role trust policies and Amazon S3 bucket policies
. In services that support resource-based policies, service administrators can use them to control access to a specific resource. For the resource where the policy is attached, the policy defines what actions a specified principal can perform on that resource and under what conditions. You must specify a principal in a resource-based policy. Principals can include accounts, users, roles, federated users, or AWS services. To enable cross-account access, you can specify an entire account or IAM entities in another account as the principal in a resource-based policy. For more information, see Cross account resource access in IAM in the IAM User Guide
. The Amazon S3 service supports bucket policies
, access points policies
, and access grants
: Bucket policies are resource-based policies that are attached to an Amazon S3 bucket. A bucket policy defines which principals can perform actions on the bucket. Access point policies are resource-based polices that are evaluated in conjunction with the underlying bucket policy. Access grants are a simplified model for defining access permissions to data in Amazon S3 by prefix, bucket, or object. For information about S3 Access Grants, see Managing access with S3 Access Grants
. Principals for bucket policies The Principal element specifies the user, account, service, or other entity that is either allowed or denied access to a resource. The following are examples of specifying Principal
. For more information, see Principal in the IAM User Guide
. Grant permissions to an AWS account To grant permissions to an AWS account, identify the account using the following format. "AWS":"
account-ARN
" The following are examples. "Principal":
{
"AWS":"arn:aws:iam::
AccountIDWithoutHyphens
:root"} "Principal":
{
"AWS":["arn:aws:iam::
AccountID1WithoutHyphens
:root","arn:aws:iam::
AccountID2WithoutHyphens
:root"]} Note
The examples above grant permissions to the root user, which delegates permissions to the account level. However, IAM policies are still required on the specific roles and users in the account. Grant permissions to an IAM user To grant permission to an IAM user within your account, you must provide an "AWS":"
user-ARN
" name-value pair. "Principal":
{
"AWS":"arn:aws:iam::
account-number-without-hyphens
:user/
username
"} For detailed examples that provide step-by-step instructions, see Example 1: Bucket owner granting its users bucket permissions and Example 3: Bucket owner granting permissions to objects it does not own
. Note
If an IAM identity is deleted after you update your bucket policy, the bucket policy will show a unique identifier in the principal element instead of an ARN. These unique IDs are never reused, so you can safely remove principals with unique identifiers from all of your policy statements. For more information about unique identifiers, see IAM identifiers in the IAM User Guide
. Grant anonymous permissions Warning
Use caution when granting anonymous access to your Amazon S3 bucket. When you grant anonymous access, anyone in the world can access your bucket. We highly recommend that you never grant any kind of anonymous write access to your S3 bucket. To grant permission to everyone, also referred as anonymous access, you set the wildcard (
"*"
) as the Principal value. For example, if you configure your bucket as a website, you want all the objects in the bucket to be publicly accessible. "Principal":"*" "Principal":
{
"AWS":"*"} Using "Principal": "*" with an Allow effect in a resource-based policy allows anyone, even if theyâre not signed in to AWS, to access your resource. Using "Principal" : { "AWS" : "*" } with an Allow effect in a resource-based policy allows any root user, IAM user, assumed-role session, or federated user in any account in the same partition to access your resource. For anonymous users, these two methods are equivalent. For more information, see All principals in the IAM User Guide
. You cannot use a wildcard to match part of a principal name or ARN. Important
In AWS access control policies, the Principals "*" and {
"AWS": "*"} behave identically. Restrict resource permissions You can also use resource policy to restrict access to resources that would otherwise be available to IAM principals. Use a Deny statement to prevent access. The following example blocks access if a secure transport protocol isnât used: JSON { "Version":"2012-10-17", "Statement": [ { "Sid": "DenyBucketAccessIfSTPNotUsed", "Effect": "Deny", "Principal": "*", "Action": "s3:*", "Resource": "arn:aws:s3:::
amzn-s3-demo-bucket
", "Condition": { "Bool": { "aws:SecureTransport": "false" } } } ]
} Using "Principal": "*" so that this restriction applies to everyone is a best practice for this policy, instead of attempting to deny access only to specific accounts or principals using this method. Require access through CloudFront URLs You can require that your users access your Amazon S3 content only by using CloudFront URLs instead of Amazon S3 URLs. To do this, create a CloudFront origin access control (OAC). Then, change the permissions on your S3 data. In your bucket policy, you can set CloudFront as the Principal as follows: "Principal":
{
"Service":"cloudfront.amazonaws.com"} Use a Condition element in the policy to allow CloudFront to access the bucket only when the request is on behalf of the CloudFront distribution that contains the S3 origin. "Condition": { "StringEquals": { "AWS:SourceArn": "arn:aws:cloudfront::
111122223333
:distribution/
CloudFront-distribution-ID
" } } For more information about requiring S3 access through CloudFront URLs, see Restricting access to an Amazon Simple Storage Service origin in the Amazon CloudFront Developer Guide
. For more information about the security and privacy benefits of using Amazon CloudFront, see Configuring secure access and restricting access to content
. Resource-based policy examples for Amazon S3 To view policy examples for Amazon S3 buckets, see Bucket policies for Amazon S3
.
To view policy examples for access points, see Configuring IAM policies for using access points
. Policy actions for Amazon S3 Supports policy actions: Yes Administrators can use AWS JSON policies to specify who has access to what. That is, which principal can perform actions on what resources
, and under what conditions
. The Action element of a JSON policy describes the actions that you can use to allow or deny access in a policy. Include actions in a policy to grant permissions to perform the associated operation. The following shows different types of mapping relationship between S3 API operations and the required policy actions. One-to-one mapping with the same name. For example, to use the PutBucketPolicy API operation, the s3:PutBucketPolicy policy action is required.
One-to-one mapping with different names. For example, to use the ListObjectsV2 API operation, the s3:ListBucket policy action is required.
One-to-many mapping. For example, to use the HeadObject API operation, the s3:GetObject is required. Also, when you use S3 Object Lock and want to get an object's Legal Hold status or retention settings, the corresponding s3:GetObjectLegalHold or s3:GetObjectRetention policy actions are also required before you can use the HeadObject API operation.
Many-to-one mapping. For example, to use the ListObjectsV2 or HeadBucket API operations, the s3:ListBucket policy action is required. To see a list of Amazon S3 actions for use in policies, see Actions defined by Amazon S3 in the Service Authorization Reference
. For a complete list of Amazon S3 API operations, see Amazon S3 API Actions in the Amazon Simple Storage Service API Reference
. For more information about the permissions to S3 API operations by S3 resource types, see Required permissions for Amazon S3 API operations
. Policy actions in Amazon S3 use the following prefix before the action: s3 To specify multiple actions in a single statement, separate them with commas. "Action": [ "s3:
action1
", "s3:
action2
" ] Bucket operations Bucket operations are S3 API operations that operate on the bucket resource type. For example, CreateBucket
, ListObjectsV2
, and PutBucketPolicy
. S3 policy actions for bucket operations require the Resource element in bucket policies or IAM identity-based policies to be the S3 bucket type Amazon Resource Name (ARN) identifier in the following example format. "Resource": "arn:aws:s3:::
amzn-s3-demo-bucket
" The following bucket policy grants the user Akua with account 12345678901 the s3:ListBucket permission to perform the ListObjectsV2 API operation and list objects in an S3 bucket. JSON { "Version":"2012-10-17", "Statement": [ { "Sid": "Allow Akua to list objects in the bucket", "Effect": "Allow", "Principal": { "AWS": "arn:aws:iam::
111122223333
:user/Akua" }, "Action": [ "s3:ListBucket" ], "Resource": "arn:aws:s3:::
amzn-s3-demo-bucket
" } ]
} Bucket operations in policies for access points for general purpose buckets Permissions granted in an access point for general purpose buckets policy are effective only if the underlying bucket allows the same permissions. When you use S3 Access Points, you must delegate access control from the bucket to the access point or add the same permissions in the access point policies to the underlying bucket's policy. For more information, see Configuring IAM policies for using access points
. In access point policies, S3 policy actions for bucket operations require you to use the access point ARN for the Resource element in the following format. "Resource": "arn:aws:s3:
us-west-2
:
123456789012
:accesspoint/
example-access-point
" The following access point policy grants the user Akua with account 12345678901 the s3:ListBucket permission to perform the ListObjectsV2 API operation through the S3 access point named example-access-point
. This permission allows Akua to list the objects in the bucket that's associated with example-access-point
. JSON { "Version":"2012-10-17", "Statement": [ { "Sid": "AllowAkuaToListObjectsInBucketThroughAccessPoint", "Effect": "Allow", "Principal": { "AWS": "arn:aws:iam::
111122223333
:user/
Akua
" }, "Action": [ "s3:ListBucket" ], "Resource": "arn:aws:s3:
us-west-2
:
111122223333
:accesspoint/
example-access-point
" } ]
} Note
Not all bucket operations are supported by access points for general purpose buckets. For more information, see Access points compatibility with S3 operations
. Bucket operations in policies for access points for directory buckets Permissions granted in an access points for directory buckets policy are effective only if the underlying bucket allows the same permissions. When you use S3 Access Points, you must delegate access control from the bucket to the access point or add the same permissions in the access point policies to the underlying bucket's policy. For more information, see Configuring IAM policies for using access points for directory buckets
. In access point policies, S3 policy actions for bucket operations require you to use the access point ARN for the Resource element in the following format. "Resource": "arn:aws:s3:
us-west-2
:
123456789012
:accesspoint/
example-access-point--usw2-az1--xa-s3
" The following access point policy grants the user Akua with account 12345678901 the s3:ListBucket permission to perform the ListObjectsV2 API operation through the access point named example-access-point--usw2-az1--xa-s3
. This permission allows Akua to list the objects in the bucket that's associated with example-access-point--usw2-az1--xa-s3
. JSON { "Version":"2012-10-17", "Statement": [ { "Sid": "AllowAkuaToListObjectsInTheBucketThroughAccessPoint", "Effect": "Allow", "Principal": { "AWS": "arn:aws:iam::111122223333:user/Akua" }, "Action": [ "s3:ListBucket" ], "Resource": "arn:aws:s3express:us-east-1:111122223333:accesspoint/example-access-point-usw2-az1-xa-s3" } ]
} Note
Not all bucket operations are supported by access points for directory buckets. For more information, see Object operations for access points for directory buckets
. Object operations Object operations are S3 API operations that act upon the object resource type. For example, GetObject
, PutObject
, and DeleteObject
. S3 policy actions for object operations require the Resource element in policies to be the S3 object ARN in the following example formats. "Resource": "arn:aws:s3:::
amzn-s3-demo-bucket
/*" "Resource": "arn:aws:s3:::
amzn-s3-demo-bucket
/
prefix
/*" Note
The object ARN must contain a forward slash after the bucket name, as seen in the previous examples. The following bucket policy grants the user Akua with account 12345678901 the s3:PutObject permission. This permission allows Akua to use the PutObject API operation to upload objects to the S3 bucket named amzn-s3-demo-bucket
. JSON { "Version":"2012-10-17", "Statement": [ { "Sid": "Allow Akua to upload objects", "Effect": "Allow", "Principal": { "AWS": "arn:aws:iam::
111122223333
:user/
Akua
" }, "Action": [ "s3:PutObject" ], "Resource": "arn:aws:s3:::
amzn-s3-demo-bucket
/*" } ]
} Object operations in access point policies When you use S3 Access Points to control access to object operations, you can use access point policies. When you use access point policies, S3 policy actions for object operations require you to use the access point ARN for the Resource element in the following format: arn:aws:s3:
region
:
account-id
:accesspoint/
access-point-name
/object/
resource
. For object operations that use access points, you must include the /object/ value after the whole access point ARN in the Resource element. Here are some examples. "Resource": "arn:aws:s3:
us-west-2
:
123456789012
:accesspoint/
example-access-point
/object/*" "Resource": "arn:aws:s3:
us-west-2
:
123456789012
:accesspoint/
example-access-point
/object/
prefix
/*" The following access point policy grants the user Akua with account 12345678901 the s3:GetObject permission. This permission allows Akua to perform the GetObject API operation through the access point named example-access-point on all objects in the bucket that's associated with the access point. JSON { "Version":"2012-10-17", "Statement": [ { "Sid": "Allow Akua to get objects through access point", "Effect": "Allow", "Principal": { "AWS": "arn:aws:iam::
111122223333
:user/
Akua
" }, "Action": [ "s3:GetObject" ], "Resource": "arn:aws:s3:
us-east-1
:
111122223333
:accesspoint/
example-access-point
/object/*" } ]
} Note
Not all object operations are supported by access points. For more information, see Access points compatibility with S3 operations
. Object operations in policies for access points for directory buckets When you use access points for directory buckets to control access to object operations, you can use access point policies. When you use access point policies, S3 policy actions for object operations require you to use the access point ARN for the Resource element in the following format: arn:aws:s3:
region
:
account-id
:accesspoint/
access-point-name
/object/
resource
. For object operations that use access points, you must include the /object/ value after the whole access point ARN in the Resource element. Here are some examples. "Resource": "arn:aws:s3express:
us-west-2
:
123456789012
:accesspoint/
example-access-point--usw2-az1--xa-s3
/object/*" "Resource": "arn:aws:s3express:
us-west-2
:
123456789012
:accesspoint/
example-access-point--usw2-az1--xa-s3
/object/
prefix
/*" The following access point policy grants the user Akua with account 12345678901 the s3:GetObject permission. This permission allows Akua to perform the GetObject API operation through the access point named example-access-point--usw2-az1--xa-s3 on all objects in the bucket that's associated with the access point. { "Version": "2012-10-17", "Statement": [ { "Sid": "Allow Akua to get objects through access point", "Effect": "Allow", "Principal": { "AWS": "arn:aws:iam::
12345678901
:user/
Akua
" }, "Action": "s3express:CreateSession","s3:GetObject" "Resource": "arn:aws:s3:
us-west-2
:
123456789012
:accesspoint/
example-access-point--usw2-az1--xa-s3
/object/*" } ]
} Note
Not all object operations are supported by access points for directory buckets. For more information, see Object operations for access points for directory buckets
. Access point for general purpose bucket operations Access point operations are S3 API operations that operate on the accesspoint resource type. For example, CreateAccessPoint
, DeleteAccessPoint
, and GetAccessPointPolicy
. S3 policy actions for access point operations can only be used in IAM identity-based policies, not in bucket policies or access point policies. Access points operations require the Resource element to be the access point ARN in the following example format. "Resource": "arn:aws:s3:
us-west-2
:
123456789012
:accesspoint/
example-access-point
" The following IAM identity-based policy grants the s3:GetAccessPointPolicy permission to perform the GetAccessPointPolicy API operation on the S3 access point named example-access-point
. JSON { "Version":"2012-10-17", "Statement": [ { "Sid": "GrantPermissionToRetrieveTheAccessPointPolicyOfAccessPointExampleAccessPoint", "Effect": "Allow", "Action": [ "s3:GetAccessPointPolicy" ], "Resource": "arn:aws:s3:*:
123456789012
:accesspoint/
example-access-point
" } ]
} When you use Access Points, to control access to bucket operations, see Bucket operations in policies for access points for general purpose buckets
; to control access to object operations, see Object operations in access point policies
. For more information about how to configure access point policies, see Configuring IAM policies for using access points
. Access point for directory buckets operations Access point for directory buckets operations are S3 API operations that operate on the accesspoint resource type. For example, CreateAccessPoint
, DeleteAccessPoint
, and GetAccessPointPolicy
. S3 policy actions for access point operations can only be used in IAM identity-based policies, not in bucket policies or access point policies. Access points for directory buckets operations require the Resource element to be the access point ARN in the following example format. "Resource": "arn:aws:s3:
us-west-2
:
123456789012
:accesspoint/
example-access-point--usw2-az1--xa-s3
" The following IAM identity-based policy grants the s3express:GetAccessPointPolicy permission to perform the GetAccessPointPolicy API operation on the access point named example-access-point--usw2-az1--xa-s3
. JSON { "Version":"2012-10-17", "Statement": [ { "Sid": "GrantPermissionToRetrieveTheAccessPointPolicyOfAccessPointExampleAccessPointUsw2Az1XaS3", "Effect": "Allow", "Action": [ "s3express:CreateSession","s3express:GetAccessPointPolicy" ], "Resource": "arn:aws:s3:*:
111122223333
:accesspoint/
example-access-point
" } ]
} The following IAM identity-based policy grants the s3express:CreateAccessPoint permission to create an access points for directory buckets. { "Version": "2012-10-17", "Statement": [ { "Sid": "Grant CreateAccessPoint.", "Principal": "*", "Action": "s3express:CreateSession", "s3express:CreateAccessPoint""Effect": "Allow", "Resource": "*" } ]
} The following IAM identity-based policy grants the s3express:PutAccessPointScope permission to create access point scope for access points for directory buckets. { "Version": "2012-10-17", "Statement": [ { "Sid": "Grant PutAccessPointScope", "Principal": "*", "Action": "s3express:CreateSession", "s3express:CreateAccessPoint", "S3Express:PutAccessPointScope""Effect": "Allow", "Resource": "*", } ]
} When you use access points for directory buckets to control access to bucket operations, see Bucket operations in policies for access points for directory buckets
; to control access to object operations, see Object operations in policies for access points for directory buckets
. For more information about how to configure access points for directory buckets policies, see Configuring IAM policies for using access points for directory buckets
. Object Lambda Access Point operations With Amazon S3 Object Lambda, you can add your own code to Amazon S3 GET
, LIST
, and HEAD requests to modify and process data as it is returned to an application. You can make requests through an Object Lambda Access Point, which works the same as making requests through other access points. For more information, see Transforming objects with S3 Object Lambda
. For more information about how to configure policies for Object Lambda Access Point operations, see Configuring IAM policies for Object Lambda Access Points
. Multi-Region Access Point operations A Multi-Region Access Point provides a global endpoint that applications can use to fulfill requests from S3 buckets that are located in multiple AWS Region. You can use a Multi-Region Access Point to build multi-Region applications with the same architecture that's used in a single Region, and then run those applications anywhere in the world. For more information, see Managing multi-Region traffic with Multi-Region Access Points
. For more information about how to configure policies for Multi-Region Access Point operations, see Multi-Region Access Point policy examples
. Batch job operations (Batch Operations) job operations are S3 API operations that operate on the job resource type. For example, DescribeJob and CreateJob
. S3 policy actions for job operations can only be used in IAM identity-based policies, not in bucket policies. Also, job operations require the Resource element in IAM identity-based policies to be the job ARN in the following example format. "Resource": "arn:aws:s3:*:
123456789012
:job/*" The following IAM identity-based policy grants the s3:DescribeJob permission to perform the DescribeJob API operation on the S3 Batch Operations job named example-job
. JSON { "Version":"2012-10-17", "Statement": [ { "Sid": "AllowDescribingBatchOperationJob", "Effect": "Allow", "Action": [ "s3:DescribeJob" ], "Resource": "arn:aws:s3:*:
111122223333
:job/
example-job
" } ]
} S3 Storage Lens configuration operations For more information about how to configure S3 Storage Lens configuration operations, see Setting Amazon S3 Storage Lens permissions
. Account operations Account operations are S3 API operations that operate on the account level. For example, GetPublicAccessBlock (for account). Account isn't a resource type defined by Amazon S3. S3 policy actions for account operations can only be used in IAM identity-based policies, not in bucket policies. Also, account operations require the Resource element in IAM identity-based policies to be "*"
. The following IAM identity-based policy grants the s3:GetAccountPublicAccessBlock permission to perform the account-level GetPublicAccessBlock API operation and retrieve the account-level Public Access Block settings. JSON { "Version":"2012-10-17", "Statement":[ { "Sid":"AllowRetrievingTheAccountLevelPublicAccessBlockSettings", "Effect":"Allow", "Action":[ "s3:GetAccountPublicAccessBlock" ], "Resource":[ "*" ] } ]
} Policy examples for Amazon S3 To view examples of Amazon S3 identity-based policies, see Identity-based policies for Amazon S3
. To view examples of Amazon S3 resource-based policies, see Bucket policies for Amazon S3 and Configuring IAM policies for using access points
. Policy resources for Amazon S3 Supports policy resources: Yes Administrators can use AWS JSON policies to specify who has access to what. That is, which principal can perform actions on what resources
, and under what conditions
. The Resource JSON policy element specifies the object or objects to which the action applies. As a best practice, specify a resource using its Amazon Resource Name (ARN)
. For actions that don't support resource-level permissions, use a wildcard (*) to indicate that the statement applies to all resources. "Resource": "*" Some Amazon S3 API actions support multiple resources. For example, s3:GetObject accesses example-resource-1 and example-resource-2
, so a principal must have permissions to access both resources. To specify multiple resources in a single statement, separate the ARNs with commas, as shown in the following example. "Resource": [ "
example-resource-1
", "
example-resource-2
" Resources in Amazon S3 are buckets, objects, access points, or jobs. In a policy, use the Amazon Resource Name (ARN) of the bucket, object, access point, or job to identify the resource. To see a complete list of Amazon S3 resource types and their ARNs, see Resources defined by Amazon S3 in the Service Authorization Reference
. To learn with which actions you can specify the ARN of each resource, see Actions defined by Amazon S3
. For more information about the permissions to S3 API operations by S3 resource types, see Required permissions for Amazon S3 API operations
. Wildcard characters in resource ARNs You can use wildcard characters as part of the resource ARN. You can use the wildcard characters (
* and ?
) within any ARN segment (the parts separated by colons). An asterisk (
*
) represents any combination of zero or more characters, and a question mark (
?
) represents any single character. You can use multiple * or ? characters in each segment. However, a wildcard character can't span segments. The following ARN uses the * wildcard character in the relative-ID part of the ARN to identify all objects in the amzn-s3-demo-bucket bucket. arn:aws:s3:::
amzn-s3-demo-bucket
/* The following ARN uses * to indicate all S3 buckets and objects. arn:aws:s3:::* The following ARN uses both of the wildcard characters, * and ?
, in the relative-ID part. This ARN identifies all objects in buckets such as amzn-s3-demo-example1bucket
, amzn-s3-demo-example2bucket
, amzn-s3-demo-example3bucket
, and so on. arn:aws:s3:::
amzn-s3-demo-example
?
bucket
/* Policy variables for resource ARNs You can use policy variables in Amazon S3 ARNs. At policy-evaluation time, these predefined variables are replaced by their corresponding values. Suppose that you organize your bucket as a collection of folders, with one folder for each of your users. The folder name is the same as the username. To grant users permission to their folders, you can specify a policy variable in the resource ARN: arn:aws:s3:::
bucket_name
/
developers
/$
{
aws:username}/ At runtime, when the policy is evaluated, the variable $
{
aws:username} in the resource ARN is substituted with the username of the person who is making the request. Policy examples for Amazon S3 To view examples of Amazon S3 identity-based policies, see Identity-based policies for Amazon S3
. To view examples of Amazon S3 resource-based policies, see Bucket policies for Amazon S3 and Configuring IAM policies for using access points
. Policy condition keys for Amazon S3 Supports service-specific policy condition keys: Yes Administrators can use AWS JSON policies to specify who has access to what. That is, which principal can perform actions on what resources
, and under what conditions
. The Condition element specifies when statements execute based on defined criteria. You can create conditional expressions that use condition operators
, such as equals or less than, to match the condition in the policy with values in the request. To see all AWS global condition keys, see AWS global condition context keys in the IAM User Guide
. Each Amazon S3 condition key maps to the same name request header allowed by the API on which the condition can be set. Amazon S3âspecific condition keys dictate the behavior of the same name request headers. For example, the condition key s3:VersionId used to grant conditional permission for the s3:GetObjectVersion permission defines behavior of the versionId query parameter that you set in a GET Object request. To see a list of Amazon S3 condition keys, see Condition keys for Amazon S3 in the Service Authorization Reference
. To learn with which actions and resources you can use a condition key, see Actions defined by Amazon S3
. Example: Restricting object uploads to objects with a specific storage class Suppose that Account A, represented by account ID 123456789012
, owns a bucket. The Account A administrator wants to restrict Dave
, a user in Account A, so that Dave can upload objects to the bucket only if the object is stored in the STANDARD_IA storage class. To restrict object uploads to a specific storage class, the Account A administrator can use the s3:x-amz-storage-class condition key, as shown in the following example bucket policy. JSON { "Version":"2012-10-17", "Statement": [ { "Sid": "statement1", "Effect": "Allow", "Principal": { "AWS": "arn:aws:iam::
123456789012
:user/
Dave
" }, "Action": "s3:PutObject", "Resource": "arn:aws:s3:::amzn-s3-demo-bucket1/*", "Condition": { "StringEquals": { "s3:x-amz-storage-class": [ "STANDARD_IA" ] } } } ] } In the example, the Condition block specifies the StringEquals condition that is applied to the specified key-value pair, "s3:x-amz-acl":["public-read"]
. There is a set of predefined keys that you can use in expressing a condition. The example uses the s3:x-amz-acl condition key. This condition requires the user to include the x-amz-acl header with value public-read in every PutObject request. Policy examples for Amazon S3 To view examples of Amazon S3 identity-based policies, see Identity-based policies for Amazon S3
. To view examples of Amazon S3 resource-based policies, see Bucket policies for Amazon S3 and Configuring IAM policies for using access points
. ACLs in Amazon S3 Supports ACLs: Yes In Amazon S3, access control lists (ACLs) control which AWS accounts have permissions to access a resource. ACLs are similar to resource-based policies, although they do not use the JSON policy document format. Important
A majority of modern use cases in Amazon S3 no longer require the use of ACLs. For information about using ACLs to control access in Amazon S3, see Managing access with ACLs
. ABAC with Amazon S3 Supports ABAC (tags in policies): Partial Attribute-based access control (ABAC) is an authorization strategy that defines permissions based on attributes called tags. You can attach tags to IAM entities and AWS resources, then design ABAC policies to allow operations when the principal's tag matches the tag on the resource. To control access based on tags, you provide tag information in the condition element of a policy using the aws:ResourceTag/
key-name
, aws:RequestTag/
key-name
, or aws:TagKeys condition keys. If a service supports all three condition keys for every resource type, then the value is Yes for the service. If a service supports all three condition keys for only some resource types, then the value is Partial
. For more information about ABAC, see Define permissions with ABAC authorization in the IAM User Guide
. To view a tutorial with steps for setting up ABAC, see Use attribute-based access control (ABAC) in the IAM User Guide
. For information about resources that support ABAC in Amazon S3, see Using tags for attribute-based access control (ABAC)
. To view example identity-based policies for limiting access to S3 Batch Operations jobs based on tags, see Controlling permissions for Batch Operations using job tags
. ABAC and object tags In ABAC policies, objects use s3: tags instead of aws: tags. To control access to objects based on object tags, you provide tag information in the Condition element of a policy using the following tags: s3:ExistingObjectTag/
tag-key s3:RequestObjectTagKeys s3:RequestObjectTag/
tag-key For information about using object tags to control access, including example permission policies, see Tagging and access control policies
. Using temporary credentials with Amazon S3 Supports temporary credentials: Yes Temporary credentials provide short-term access to AWS resources and are automatically created when you use federation or switch roles. AWS recommends that you dynamically generate temporary credentials instead of using long-term access keys. For more information, see Temporary security credentials in IAM and AWS services that work with IAM in the IAM User Guide
. Forward access sessions for Amazon S3 Supports forward access sessions (FAS): Yes Forward access sessions (FAS) use the permissions of the principal calling an AWS service, combined with the requesting AWS service to make requests to downstream services. For policy details when making FAS requests, see Forward access sessions
. FAS is used by Amazon S3 to make calls to AWS KMS to decrypt an object when SSE-KMS was used to encrypt it. For more information, see Using server-side encryption with AWS KMS keys (SSE-KMS)
. S3 Access Grants also uses FAS. After you create an access grant to your S3 data for a particular identity, the grantee requests a temporary credential from S3 Access Grants. S3 Access Grants obtains a temporary credential for the requester from AWS STS and vends the credential to the requester. For more information, see Request access to Amazon S3 data through S3 Access Grants
. Service roles for Amazon S3 Supports service roles: Yes A service role is an IAM role that a service assumes to perform actions on your behalf. An IAM administrator can create, modify, and delete a service role from within IAM. For more information, see Create a role to delegate permissions to an AWS service in the IAM User Guide
. Warning
Changing the permissions for a service role might break Amazon S3 functionality. Edit service roles only when Amazon S3 provides guidance to do so. Service-linked roles for Amazon S3 Supports service-linked roles: Partial A service-linked role is a type of service role that is linked to an AWS service. The service can assume the role to perform an action on your behalf. Service-linked roles appear in your AWS account and are owned by the service. An IAM administrator can view, but not edit the permissions for service-linked roles. Amazon S3 supports service-linked roles for Amazon S3 Storage Lens. For details about creating or managing Amazon S3 service-linked roles, see Using service-linked roles for Amazon S3 Storage Lens
. Amazon S3 Service as a Principal Service name in the policy S3 feature More information s3.amazonaws.com S3 Replication Setting up live replication overview s3.amazonaws.com S3 event notifications Amazon S3 Event Notifications s3.amazonaws.com S3 Inventory Cataloging and analyzing your data with S3 Inventory access-grants.s3.amazonaws.com S3 Access Grants Register a location batchoperations.s3.amazonaws.com S3 Batch Operations Granting permissions for Batch Operations logging.s3.amazonaws.com S3 Server Access Logging Enabling Amazon S3 server access logging storage-lens.s3.amazonaws.com S3 Storage Lens Viewing Amazon S3 Storage Lens metrics using a data export Javascript is disabled or is unavailable in your browser.
To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.
Document Conventions
Identity and Access Management (IAM)
Request authorization ============================== Security best practices for Amazon S3
Important
Starting in April 2026, AWS will disable server-side encryption with customer-provided keys (SSE-C) for all new buckets. In addition, SSE-C encryption will be disabled for all existing buckets in AWS accounts that do not have any SSE-C encrypted data. With these changes, the few applications that need SSE-C encryption must deliberately enable the use SSE-C via the PutBucketEncryption API after creating the bucket. In these cases, you might need to update automation scripts, CloudFormation templates, or other infrastructure configuration tools to configure these settings. For more information, see the AWS Storage Blog post
.
Amazon S3 provides a number of security features to consider as you develop and implement your own security policies. The following best practices are general guidelines and don't represent a complete security solution. Because these best practices might not be appropriate or sufficient for your environment, treat them as helpful recommendations rather than prescriptions. Topics
Amazon S3 security best practices
Amazon S3 monitoring and auditing best practices
Monitoring data security with managed AWS security services Amazon S3 security best practices The following best practices for Amazon S3 can help prevent security incidents. Disable access control lists (ACLs) S3 Object Ownership is an Amazon S3 bucket-level setting that you can use to control ownership of objects uploaded to your bucket and to disable or enable ACLs. By default, Object Ownership is set to the bucket owner enforced setting and all ACLs are disabled. When ACLs are disabled, the bucket owner owns all the objects in the bucket and manages access to data exclusively using access management policies. A majority of modern use cases in Amazon S3 no longer require the use of access control lists (ACLs)
. We recommend that you disable ACLs, except in circumstances where you must control access for each object individually. To disable ACLs and take ownership of every object in your bucket, apply the bucket owner enforced setting for S3 Object Ownership. When you disable ACLs, you can easily maintain a bucket with objects uploaded by different AWS accounts. When ACLs are disabled access control for your data is based on policies, such as the following: AWS Identity and Access Management (IAM) user policies S3 bucket policies Virtual private cloud (VPC) endpoint policies AWS Organizations service control policies (SCPs) AWS Organizations resource control policies (RCPs) Disabling ACLs simplifies permissions management and auditing. ACLs are disabled for new buckets by default. You can also disable ACLs for existing buckets. If you have an existing bucket that already has objects in it, after you disable ACLs, the object and bucket ACLs are no longer part of the access-evaluation process. Instead, access is granted or denied on the basis of policies. Before you disable ACLs, make sure that you do the following: Review your bucket policy to ensure that it covers all the ways that you intend to grant access to your bucket outside of your account. Reset your bucket ACL to the default (full control to the bucket owner). After you disable ACLs, the following behaviors occur: Your bucket accepts only PUT requests that do not specify an ACL or PUT requests with bucket owner full control ACLs. These ACLs include the bucket-owner-full-control canned ACL or equivalent forms of this ACL that are expressed in XML. Existing applications that support bucket owner full control ACLs see no impact. PUT requests that contain other ACLs (for example, custom grants to certain AWS accounts) fail and return an HTTP status code 400 (Bad Request) with the error code AccessControlListNotSupported
. For more information, see Controlling ownership of objects and disabling ACLs for your bucket
. Ensure that your Amazon S3 buckets use the correct policies and are not publicly accessible Unless you explicitly require anyone on the internet to be able to read or write to your S3 bucket, make sure that your S3 bucket is not public. The following are some of the steps that you can take to block public access: Use S3 Block Public Access. With S3 Block Public Access, you can easily set up centralized controls to limit public access to your Amazon S3 resources. These centralized controls are enforced regardless of how the resources are created. For organizations managing multiple AWS accounts, you can now use organization-level enforcement through AWS Organizations to centrally manage S3 Block Public Access settings across your entire organization with a single policy configuration. For more information, see Blocking public access to your Amazon S3 storage
. Identify Amazon S3 bucket policies that allow a wildcard identity such as "Principal": "*" (which effectively means "anyone"). Also look for policies that allow a wildcard action "*" (which effectively allows the user to perform any action in the Amazon S3 bucket). Similarly, look for Amazon S3 bucket access control lists (ACLs) that provide read, write, or full-access to "Everyone" or "Any authenticated AWS user." Use the ListBuckets API operation to scan all of your Amazon S3 buckets. Then use GetBucketAcl
, GetBucketWebsite
, and GetBucketPolicy to determine whether each bucket has compliant access controls and a compliant configuration. Use AWS Trusted Advisor to inspect your Amazon S3 implementation. Consider implementing ongoing detective controls by using the s3-bucket-public-read-prohibited and s3-bucket-public-write-prohibited managed AWS Config Rules. For organizations with multiple AWS accounts, consider using organization-level Block Public Access management: Centralized policy management: Use AWS Organizations to create a single S3 Block Public Access policy that automatically applies to all member accounts or selected organizational units (OUs). Automatic inheritance: When you attach the policy at the root or OU level, new member accounts automatically inherit the Block Public Access settings without individual account setup. Simplified compliance: Organization-level policies eliminate the need to maintain complex Service Control Policies (SCPs) for Block Public Access enforcement and reduce operational overhead of managing individual account configurations. Audit capabilities: Use AWS CloudTrail to monitor policy attachment and enforcement across member accounts for compliance tracking. For more information, see Identity and Access Management for Amazon S3
. Disable server-side encryption with customer-provided keys (SSE-C) to your buckets Most modern use cases in Amazon S3 no longer use SSE-C because it lacks the flexibility of server-side encryption with Amazon S3 managed keys (SSE-S3) or server-side encryption with AWS KMS keys (SSE-KMS). SSE-C's requirement to provide the encryption key each time you interact with your SSE-C encrypted data makes it impractical to share your SSE-C key with other users, roles, or AWS services who read data from your S3 buckets in order to operate on your data. To limit the server-side encryption types you can use in your general purpose buckets, you can choose to block SSE-C write requests by updating your default encryption configuration for your buckets. This bucket-level configuration blocks requests to upload objects that specify SSE-C. When SSE-C is blocked for a bucket, any PutObject
, CopyObject
, PostObject
, or Multipart Upload or replication requests that specify SSE-C encryption will be rejected with an HTTP 403 AccessDenied error. To learn more about blocking SSE-C, see Blocking or unblocking SSE-C for a general purpose bucket
. Create bucket names that aren't predictable Bucket names must be unique across all AWS accounts in all of the AWS Regions within a partition. Once an AWS account creates a bucket, that bucket name can't be used by another AWS account in the same partition until the bucket is deleted. We recommend that you create bucket names that aren't predictable. Don't write code that assumes your chosen bucket name is available unless you have already created the bucket. One method for creating bucket names that aren't predictable is to append a Globally Unique Identifier (GUID) to your bucket name, for example, amazon-s3-demo-bucket-a1b2c3d4-5678-90ab-cdef-example11111
. For more information, see Creating a bucket that uses a GUID in the bucket name
. We recommend that you don't delete your buckets. All AWS Accounts now have a default bucket quota of 10,000 buckets, reducing the need to delete empty buckets from your account. If you delete a bucket, be aware that another AWS account in the same partition can use the same bucket name for a new bucket and can therefore potentially receive requests that are intended for the deleted bucket. If you want to prevent this, or if you want to continue to use the same bucket name, don't delete the bucket. We recommend that you empty the bucket and keep it. Instead of deleting the bucket, block any bucket requests as needed. For buckets no longer in active use, we recommend emptying the bucket of all objects to minimize costs while retaining the bucket itself. For more information, see Deleting a general purpose bucket
. Implement least privilege access When granting permissions, you decide who is getting what permissions to which Amazon S3 resources. You enable specific actions that you want to allow on those resources. Therefore, we recommend that you grant only the permissions that are required to perform a task. Implementing least privilege access is fundamental in reducing security risk and the impact that could result from errors or malicious intent. The following tools are available to implement least privilege access: Policy actions for Amazon S3 and Permissions Boundaries for IAM Entities How Amazon S3 works with IAM Access control list (ACL) overview For guidance on what to consider when choosing one or more of the preceding mechanisms, see Identity and Access Management for Amazon S3
. Use IAM roles for applications and AWS services that require Amazon S3 access In order for applications running on Amazon EC2 or other AWS services to access Amazon S3 resources, they must include valid AWS credentials in their AWS API requests. We recommend not storing AWS credentials directly in the application or Amazon EC2 instance. These are long-term credentials that are not automatically rotated and could have a significant business impact if they are compromised. Instead, use an IAM role to manage temporary credentials for applications or services that need to access Amazon S3. When you use a role, you don't have to distribute long-term credentials (such as a username and password or access keys) to an Amazon EC2 instance or AWS service, such as AWS Lambda. The role supplies temporary permissions that applications can use when they make calls to other AWS resources. For more information, see the following topics in the IAM User Guide
: IAM Roles Common Scenarios for Roles: Users, Applications, and Services Consider encryption of data at rest You have the following options for protecting data at rest in Amazon S3: Server-side encryption â All Amazon S3 buckets have encryption configured by default, and all new objects that are uploaded to an S3 bucket are automatically encrypted at rest. Server-side encryption with Amazon S3 managed keys (SSE-S3) is the default encryption configuration for every bucket in Amazon S3. To use a different type of encryption, you can either specify the type of server-side encryption to use in your S3 PUT requests, or you can set the default encryption configuration in the destination bucket. Amazon S3 also provides these server-side encryption options: Server-side encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS) Dual-layer server-side encryption with AWS Key Management Service (AWS KMS) keys (DSSE-KMS) Server-side encryption with customer-provided keys (SSE-C) For more information, see Protecting data with server-side encryption
. Client-side encryption â Encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools. As with server-side encryption, client-side encryption can help reduce risk by encrypting the data with a key that is stored in a different mechanism than the mechanism that stores the data itself. Amazon S3 provides multiple client-side encryption options. For more information, see Protecting data by using client-side encryption
. Enforce encryption of data in transit You can use HTTPS (TLS) to help prevent potential attackers from eavesdropping on or manipulating network traffic by using person-in-the-middle or similar attacks. We recommend allowing only encrypted connections over HTTPS (TLS) by using the aws:SecureTransport condition in your Amazon S3 bucket policies. For more information, see the example S3 bucket policy Managing access based on HTTP or HTTPS requests
. In addition to denying HTTP requests, we recommend that you set Amazon CloudWatch alarms on tlsDetails.tlsVersion NOT EXISTS that alert you if HTTP access attempts are made on your content. For more information on how to configure Amazon CloudWatch alarms, see Creating CloudWatch alarms for CloudTrail events: examples and CloudTrail record contents in the AWS CloudTrail User Guide
. Important
We recommend that your application not pin Amazon S3 TLS certificates as AWS doesnât support pinning of publicly-trusted certificates. S3 automatically renews certificates and renewal can happen any time before certificate expiry. Renewing a certificate generates a new public-private key pair. If youâve pinned an S3 certificate which has been recently renewed with a new public key, you wonât be able to connect to S3 until your application uses the new certificate. Also consider implementing ongoing detective controls by using the s3-bucket-ssl-requests-only managed AWS Config rule. Consider using S3 Object Lock With S3 Object Lock, you can store objects by using a "Write Once Read Many" (WORM) model. S3 Object Lock can help prevent accidental or inappropriate deletion of data. For example, you can use S3 Object Lock to help protect your AWS CloudTrail logs. For more information, see Locking objects with Object Lock
. Enable S3 Versioning S3 Versioning is a means of keeping multiple variants of an object in the same bucket. You can use versioning to preserve, retrieve, and restore every version of every object stored in your bucket. With versioning, you can easily recover from both unintended user actions and application failures. Also consider implementing ongoing detective controls by using the s3-bucket-versioning-enabled managed AWS Config rule. For more information, see Retaining multiple versions of objects with S3 Versioning
. Consider using S3 Cross-Region Replication Although Amazon S3 stores your data across multiple geographically diverse Availability Zones by default, compliance requirements might dictate that you store data at even greater distances. With S3 Cross-Region Replication (CRR), you can replicate data between distant AWS Regions to help satisfy these requirements. CRR enables automatic, asynchronous copying of objects across buckets in different AWS Regions. For more information, see Replicating objects within and across Regions
. Note
CRR requires both the source and destination S3 buckets to have versioning enabled. Also consider implementing ongoing detective controls by using the s3-bucket-replication-enabled managed AWS Config rule. Consider using VPC endpoints for Amazon S3 access A virtual private cloud (VPC) endpoint for Amazon S3 is a logical entity within a VPC that allows connectivity only to Amazon S3. VPC endpoints can help prevent traffic from traversing the open internet. VPC endpoints for Amazon S3 provide multiple ways to control access to your Amazon S3 data: You can control the requests, users, or groups that are allowed through a specific VPC endpoint by using S3 bucket policies. You can control which VPCs or VPC endpoints have access to your S3 buckets by using S3 bucket policies. You can help prevent data exfiltration by using a VPC that does not have an internet gateway. For more information, see Controlling access from VPC endpoints with bucket policies
. Use managed AWS security services to monitor data security Several managed AWS security services can help you identify, assess, and monitor security and compliance risks for your Amazon S3 data. These services can also help you protect your data from those risks. These services include automated detection, monitoring, and protection capabilities that are designed to scale from Amazon S3 resources for a single AWS account to resources for organizations spanning thousands of accounts. For more information, see Monitoring data security with managed AWS security services
. Amazon S3 monitoring and auditing best practices The following best practices for Amazon S3 can help detect potential security weaknesses and incidents. Identify and audit all of your Amazon S3 buckets Identification of your IT assets is a crucial aspect of governance and security. You need to have visibility of all your Amazon S3 resources to assess their security posture and take action on potential areas of weakness. To audit your resources, we recommend doing the following: Use Tag Editor to identify and tag security-sensitive or audit-sensitive resources, then use those tags when you need to search for these resources. For more information, see Searching for Resources to Tag in the Tagging AWS Resources User Guide
. Use S3 Inventory to audit and report on the replication and encryption status of your objects for business, compliance, and regulatory needs. For more information, see Cataloging and analyzing your data with S3 Inventory
. Create resource groups for your Amazon S3 resources. For more information, see What are resource groups? in the AWS Resource Groups User Guide
. Implement monitoring by using AWS monitoring tools Monitoring is an important part of maintaining the reliability, security, availability, and performance of Amazon S3 and your AWS solutions. AWS provides several tools and services to help you monitor Amazon S3 and your other AWS services. For example, you can monitor Amazon CloudWatch metrics for Amazon S3, particularly the PutRequests
, GetRequests
, 4xxErrors
, and DeleteRequests metrics. For more information, see Monitoring metrics with Amazon CloudWatch and Logging and monitoring in Amazon S3
. For a second example, see Example: Amazon S3 Bucket Activity
. This example describes how to create a CloudWatch alarm that is triggered when an Amazon S3 API call is made to PUT or DELETE a bucket policy, a bucket lifecycle, or a bucket replication configuration, or to PUT a bucket ACL. Enable Amazon S3 server access logging Server access logging provides detailed records of the requests that are made to a bucket. Server access logs can assist you in security and access audits, help you learn about your customer base, and understand your Amazon S3 bill. For instructions on enabling server access logging, see Logging requests with server access logging
. Also consider implementing ongoing detective controls by using the s3-bucket-logging-enabled AWS Config managed rule. Use AWS CloudTrail AWS CloudTrail provides a record of actions taken by a user, a role, or an AWS service in Amazon S3. You can use information collected by CloudTrail to determine the following: The request that was made to Amazon S3 The IP address from which the request was made Who made the request When the request was made Additional details about the request For example, you can identify CloudTrail entries for PUT actions that affect data access, in particular PutBucketAcl
, PutObjectAcl
, PutBucketPolicy
, and PutBucketWebsite
. When you set up your AWS account, CloudTrail is enabled by default. You can view recent events in the CloudTrail console. To create an ongoing record of activity and events for your Amazon S3 buckets, you can create a trail in the CloudTrail console. For more information, see Logging data events in the AWS CloudTrail User Guide
. When you create a trail, you can configure CloudTrail to log data events. Data events are records of resource operations performed on or within a resource. In Amazon S3, data events record object-level API activity for individual buckets. CloudTrail supports a subset of Amazon S3 object-level API operations, such as GetObject
, DeleteObject
, and PutObject
. For more information about how CloudTrail works with Amazon S3, see Logging Amazon S3 API calls using AWS CloudTrail
. In the Amazon S3 console, you can also configure your S3 buckets to Enabling CloudTrail event logging for S3 buckets and objects
. AWS Config provides a managed rule (
cloudtrail-s3-dataevents-enabled
) that you can use to confirm that at least one CloudTrail trail is logging data events for your S3 buckets. For more information, see cloudtrail-s3-dataevents-enabled in the AWS Config Developer Guide
. Enable AWS Config Several of the best practices listed in this topic suggest creating AWS Config rules. AWS Config helps you to assess, audit, and evaluate the configurations of your AWS resources. AWS Config monitors resource configurations so that you can evaluate the recorded configurations against the desired secure configurations. With AWS Config, you can do the following: Review changes in configurations and relationships between AWS resources Investigate detailed resource-configuration histories Determine your overall compliance against the configurations specified in your internal guidelines Using AWS Config can help you simplify compliance auditing, security analysis, change management, and operational troubleshooting. For more information, see Setting Up AWS Config with the Console in the AWS Config Developer Guide
. When specifying the resource types to record, ensure that you include Amazon S3 resources. Important
AWS Config managed rules only supports general purpose buckets when evaluating Amazon S3 resources. AWS Config doesnât record configuration changes for directory buckets. For more information, see AWS Config Managed Rules and List of AWS Config Managed Rules in the AWS Config Developer Guide
. For an example of how to use AWS Config, see How to Use AWS Config to Monitor for and Respond to Amazon S3 Buckets Allowing Public Access on the AWS Security Blog
. Use S3 Storage Lens S3 Storage Lens is a cloud-storage analytics feature that you can use to gain organization-wide visibility into object-storage usage and activity. S3 Storage Lens also analyzes metrics to deliver contextual recommendations that you can use to optimize storage costs and apply best practices for protecting your data. With S3 Storage Lens, you can use metrics to generate summary insights, such as finding out how much storage you have across your entire organization or which are the fastest-growing buckets and prefixes. You can also use S3 Storage Lens metrics to identify cost-optimization opportunities, implement data-protection and access-management best practices, and improve the performance of application workloads. For example, you can identify buckets that don't have S3 Lifecycle rules to abort incomplete multipart uploads that are more than 7 days old. You can also identify buckets that aren't following data-protection best practices, such as using S3 Replication or S3 Versioning. For more information, see Understanding Amazon S3 Storage Lens
. Monitor AWS security advisories We recommend that you regularly check the security advisories posted in Trusted Advisor for your AWS account. In particular, look for warnings about Amazon S3 buckets with "open access permissions." You can do this programmatically by using describe-trusted-advisor-checks
. Further, actively monitor the primary email address that's registered to each of your AWS accounts. AWS uses this email address to contact you about emerging security issues that might affect you. AWS operational issues with broad impact are posted on the AWS Health Dashboard - Service health
. Operational issues are also posted to individual accounts through the AWS Health Dashboard. For more information, see the AWS Health documentation
. Monitoring data security with managed AWS security services Several managed AWS security services can help you identify, assess, and monitor security and compliance risks for your Amazon S3 data. They can also help you protect your data from those risks. These services include automated detection, monitoring, and protection capabilities that are designed to scale from Amazon S3 resources for a single AWS account to resources for organizations spanning thousands of AWS accounts. AWS detection and response services can help you identify potential security misconfigurations, threats, or unexpected behaviors, so that you can quickly respond to potentially unauthorized or malicious activity in your environment. AWS data protection services can help you monitor and protect your data, accounts, and workloads from unauthorized access. They can also help you discover sensitive data, such as personally identifiable information (PII), in your Amazon S3 data estate. To help you identify and evaluate data security and compliance risks, managed AWS security services generate findings to notify you of potential security events or issues with your Amazon S3 data. The findings provide relevant details that you can use to investigate, assess, and act upon these risks according to your incident-response workflows and policies. You can access findings data directly by using each service. You can also send the data to other applications, services, and systems, such as your security incident and event management system (SIEM). To monitor the security of your Amazon S3 data, consider using these managed AWS security services. Amazon GuardDuty Amazon GuardDuty is a threat-detection service that continuously monitors your AWS accounts and workloads for malicious activity and delivers detailed security findings for visibility and remediation. With the S3 protection feature in GuardDuty, you can configure GuardDuty to analyze AWS CloudTrail management and data events for your Amazon S3 resources. GuardDuty then monitors those events for malicious and suspicious activity. To inform the analysis and identify potential security risks, GuardDuty uses threat-intelligence feeds and machine learning. GuardDuty can monitor different kinds of activity for your Amazon S3 resources. For example, CloudTrail management events for Amazon S3 include bucket-level operations, such as ListBuckets
, DeleteBucket
, and PutBucketReplication
. CloudTrail data events for Amazon S3 include object-level operations, such as GetObject
, ListObjects
, and PutObject
. If GuardDuty detects anomalous or potentially malicious activity, it generates a finding to notify you. For more information, see Amazon S3 Protection in Amazon GuardDuty in the Amazon GuardDuty User Guide
. Amazon Detective Amazon Detective simplifies the investigative process and helps you conduct faster, more effective security investigations. Detective provides prebuilt data aggregations, summaries, and context that can help you analyze and assess the nature and extent of possible security issues. Detective automatically extracts time-based events, such as API calls from AWS CloudTrail and Amazon VPC Flow Logs for your AWS resources. It also ingests findings generated by Amazon GuardDuty. Detective then uses machine learning, statistical analysis, and graph theory to generate visualizations that help you conduct effective security investigations more quickly. These visualizations provide a unified, interactive view of resource behaviors and the interactions between them over time. You can explore this behavior graph to examine potentially malicious actions, such as failed login attempts or suspicious API calls. You can also see how these actions affect resources, such as S3 buckets and objects. For more information, see the Amazon Detective Administration Guide
. IAM Access Analyzer AWS Identity and Access Management Access Analyzer (IAM Access Analyzer) can help you identify resources that are shared with an external entity. You can also use IAM Access Analyzer to validate IAM policies against policy grammar and best practices, and generate IAM policies based on access activity in your AWS CloudTrail logs. IAM Access Analyzer uses logic-based reasoning to analyze resource policies in your AWS environment, such as bucket policies. With IAM Access Analyzer for S3, you're alerted when an S3 bucket is configured to allow access to anyone on the internet or other AWS accounts, including accounts outside your organization. For example, IAM Access Analyzer for S3 can report that a bucket has read or write access provided through a bucket access control list (ACL), a bucket policy, a Multi-Region Access Point policy, or an access point policy. For each public or shared bucket, you receive findings that indicate the source and level of public or shared access. With these findings, you can take immediate and precise corrective action to restore bucket access to what you intended. For more information, see Reviewing bucket access using IAM Access Analyzer for S3
. Amazon Macie Amazon Macie is a security service that discovers sensitive data by using machine learning and pattern matching. Macie provides visibility into data security risks, and enables automated protection against those risks. With Macie, you can automate the discovery and reporting of sensitive data in your Amazon S3 data estate to gain a better understanding of the data that your organization stores in S3. To detect sensitive data with Macie, you can use built-in criteria and techniques that are designed to detect a large and growing list of sensitive data types for many countries and regions. These sensitive data types include multiple types of personally identifiable information (PII), financial data, and credentials data. You can also use custom criteria that you defineâregular expressions that define text patterns to match and, optionally, character sequences and proximity rules that refine the results. If Macie detects sensitive data in an S3 object, Macie generates a security finding to notify you. This finding provides information about the affected object, the types and number of occurrences of the sensitive data that Macie found, and additional details to help you investigate the affected S3 bucket and object. For more information, see the Amazon Macie User Guide
. AWS Security Hub CSPM AWS Security Hub CSPM is a security-posture management service that performs security best-practice checks, aggregates alerts and findings from multiple sources into a single format, and enables automated remediation. Security Hub CSPM collects and provides security findings data from integrated AWS Partner Network security solutions and AWS services, including Amazon Detective, Amazon GuardDuty, IAM Access Analyzer, and Amazon Macie. It also generates its own findings by running continuous, automated security checks based on AWS best practices and supported industry standards. Security Hub CSPM then correlates and consolidates findings across providers to help you prioritize and process the most significant findings. It also provides support for custom actions, which you can use to invoke responses or remediation actions for specific classes of findings. With Security Hub CSPM, you can assess the security and compliance status of your Amazon S3 resources, and you can do so as part of a broader analysis of your organization's security posture in individual AWS Regions and across multiple Regions. This includes analyzing security trends and identifying the highest-priority security issues. You can also aggregate findings from multiple AWS Regions, and monitor and process aggregated findings data from a single Region. For more information, see Amazon Simple Storage Service controls in the AWS Security Hub CSPM User Guide
. Javascript is disabled or is unavailable in your browser.
To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.
Document Conventions
Security
Data protection ============================== What is AWS Lambda?
AWS Lambda is a compute service that runs code without the need to manage servers. Your code runs, scaling up and down automatically, with pay-per-use pricing. To get started, see Create your first function
.
You can use Lambda for: File processing
: Process files automatically when uploaded to Amazon Simple Storage Service. See file processing examples for details. Long-running workflows: Use durable Lambda functions to build stateful, multi-step workflows that can run for up to one year. Perfect for order processing, approval workflows, human-in-the-loop processes, and complex data pipelines that need to remember their progress. Database operations and integration examples
: Respond to database changes and automate data workflows. See database examples for details. Scheduled and periodic tasks
: Run automated operations on a regular schedule using EventBridge. See scheduled task examples for details. Stream processing
: Process real-time data streams for analytics and monitoring. See Kinesis Data Streams for details. Web applications
: Build scalable web apps that automatically adjust to demand. Mobile backends
: Create secure API backends for mobile and web applications. IoT backends
: Handle web, mobile, IoT, and third-party API requests. See IoT for details. For pricing information, see AWS Lambda Pricing
. How Lambda works When using Lambda, you are responsible only for your code. Lambda runs your code on a high-availability compute infrastructure and manages all the computing resources, including server and operating system maintenance, capacity provisioning, automatic scaling, and logging. Because Lambda is a serverless, event-driven compute service, it uses a different programming paradigm than traditional web applications. The following model illustrates how Lambda works: You write and organize your code in Lambda functions
, which are the basic building blocks you use to create a Lambda application. You control security and access through Lambda permissions
, using execution roles to manage what AWS services your functions can interact with and what resource policies can interact with your code. Event sources and AWS services trigger your Lambda functions, passing event data in JSON format, which your functions process (this includes event source mappings). Lambda runs your code with language-specific runtimes (like Node.js and Python) in execution environments that package your runtime, layers, and extensions. Tip
To learn how to build serverless solutions
, check out the Serverless Developer Guide
. Key features Configure, control, and deploy secure applications: Environment variables modify application behavior without new code deployments. Versions safely test new features while maintaining stable production environments. Lambda layers optimize code reuse and maintenance by sharing common components across multiple functions. Code signing enforce security compliance by ensuring only approved code reaches production systems. Scale and perform reliably: Concurrency and scaling controls precisely manage application responsiveness and resource utilization during traffic spikes. Lambda SnapStart significantly reduce cold start times. Lambda SnapStart can provide as low as sub-second startup performance, typically with no changes to your function code. Response streaming optimize function performance by delivering large payloads incrementally for real-time processing. Container images package functions with complex dependencies using container workflows. Connect and integrate seamlessly: VPC networks secure sensitive resources and internal services. File system integration that shares persistent data and manage stateful operations across function invocations. Function URLs create public-facing APIs and endpoints without additional services. Lambda extensions augment functions with monitoring, security, and operational tools. Related information For information on how Lambda works, see How Lambda works
.
To start using Lambda, see Create your first Lambda function
. For a list of example applications, see Getting started with example applications and patterns
. Javascript is disabled or is unavailable in your browser.
To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.
Document Conventions
How it works ============================== Create your first Lambda function
To get started with Lambda, use the Lambda console to create a function. In a few minutes, you can create and deploy a function and test it in the console.
As you carry out the tutorial, you'll learn some fundamental Lambda concepts, like how to pass arguments to your function using the Lambda event object
. You'll also learn how to return log outputs from your function, and how to view your function's invocation logs in Amazon CloudWatch Logs.
To keep things simple, you create your function using either the Python or Node.js runtime. With these interpreted languages, you can edit function code directly in the console's built-in code editor. With compiled languages like Java and C#, you must create a deployment package on your local build machine and upload it to Lambda. To learn about deploying functions to Lambda using other runtimes, see the links in the Additional resources and next steps section.
Tip
To learn how to build serverless solutions
, check out the Serverless Developer Guide
. Prerequisites If you do not have an AWS account, complete the following steps to create one.
To sign up for an AWS account
Open https://portal.aws.amazon.com/billing/signup
. Follow the online instructions. Part of the sign-up procedure involves receiving a phone call or text message and entering a verification code on the phone keypad. When you sign up for an AWS account, an AWS account root user is created. The root user has access to all AWS services and resources in the account. As a security best practice, assign administrative access to a user, and use only the root user to perform tasks that require root user access
. AWS sends you a confirmation email after the sign-up process is
complete. At any time, you can view your current account activity and manage your account by
going to https://aws.amazon.com/ and choosing My Account
.
After you sign up for an AWS account, secure your AWS account root user, enable AWS IAM Identity Center, and create an administrative user so that you don't use the root user for everyday tasks.
Secure your AWS account root user Sign in to the AWS Management Console as the account owner by choosing Root user and entering your AWS account email address. On the next page, enter your password. For help signing in by using root user, see Signing in as the root user in the AWS Sign-In User Guide
. Turn on multi-factor authentication (MFA) for your root user. For instructions, see Enable a virtual MFA device for your AWS account root user (console) in the IAM User Guide
. Create a user with administrative access Enable IAM Identity Center. For instructions, see Enabling AWS IAM Identity Center in the AWS IAM Identity Center User Guide
. In IAM Identity Center, grant administrative access to a user. For a tutorial about using the IAM Identity Center directory as your identity source, see Configure user access with the default IAM Identity Center directory in the AWS IAM Identity Center User Guide
. Sign in as the user with administrative access To sign in with your IAM Identity Center user, use the sign-in URL that was sent to your email address when you created the IAM Identity Center user. For help signing in using an IAM Identity Center user, see Signing in to the AWS access portal in the AWS Sign-In User Guide
. Assign access to additional users In IAM Identity Center, create a permission set that follows the best practice of applying least-privilege permissions. For instructions, see Create a permission set in the AWS IAM Identity Center User Guide
. Assign users to a group, and then assign single sign-on access to the group. For instructions, see Add groups in the AWS IAM Identity Center User Guide
. Create a Lambda function with the console In this example, your function takes a JSON object containing two integer values labeled "length" and "width"
. The function multiplies these values to calculate an area and returns this as a JSON string. Your function also prints the calculated area, along with the name of its CloudWatch log group. Later in the tutorial, youâll learn to use CloudWatch Logs to view records of your functionsâ invocation. To create a Hello world Lambda function with the console
Open the Functions page of the Lambda console. Choose Create function
. Select Author from scratch
. In the Basic information pane, for Function name
, enter myLambdaFunction
. For Runtime
, choose either Node.js 24 or Python 3.14
. Leave architecture set to x86_64
, and then choose Create function
. In addition to a simple function that returns the message Hello from Lambda!
, Lambda also creates an execution role for your function. An execution role is an AWS Identity and Access Management (IAM) role that grants a Lambda function permission to access AWS services and resources. For your function, the role that Lambda creates grants basic permissions to write to CloudWatch Logs. Use the console's built-in code editor to replace the Hello world code that Lambda created with your own function code. Node.js To modify the code in the console Choose the Code tab. In the console's built-in code editor, you should see the function code that Lambda created. If you don't see the index.mjs tab in the code editor, select index.mjs in the file explorer as shown on the following diagram. Paste the following code into the index.mjs tab, replacing the code that Lambda created. export const handler = async (event, context) => { const length = event.length; const width = event.width; let area = calculateArea(length, width); console.log(`The area is $
{
area}`); console.log('CloudWatch log group: ', context.logGroupName); let data = { "area": area, }; return JSON.stringify(data); function calculateArea(length, width) { return length * width; }
}; In the DEPLOY section, choose Deploy to update your function's code: Understanding your function code Before you move to the next step, let's take a moment to look at the function code and understand some key Lambda concepts. The Lambda handler: Your Lambda function contains a Node.js function named handler
. A Lambda function in Node.js can contain more than one Node.js function, but the handler function is always the entry point to your code. When your function is invoked, Lambda runs this method. When you created your Hello world function using the console, Lambda automatically set the name of the handler method for your function to handler
. Be sure not to edit the name of this Node.js function. If you do, Lambda wonât be able to run your code when you invoke your function. To learn more about the Lambda handler in Node.js, see Define Lambda function handler in Node.js
. The Lambda event object: The function handler takes two arguments, event and context
. An event in Lambda is a JSON formatted document that contains data for your function to process. If your function is invoked by another AWS service, the event object contains information about the event that caused the invocation. For example, if your function is invoked when an object is uploaded to an Amazon Simple Storage Service (Amazon S3) bucket, the event contains the name of the bucket and the object key. In this example, youâll create an event in the console by entering a JSON formatted document with two key-value pairs. The Lambda context object: The second argument that your function takes is context
. Lambda passes the context object to your function automatically. The context object contains information about the function invocation and execution environment. You can use the context object to output information about your function's invocation for monitoring purposes. In this example, your function uses the logGroupName parameter to output the name of its CloudWatch log group. To learn more about the Lambda context object in Node.js, see Using the Lambda context object to retrieve Node.js function information
. Logging in Lambda: With Node.js, you can use console methods like console.log and console.error to send information to your function's log. The example code uses console.log statements to output the calculated area and the name of the function's CloudWatch Logs group. You can also use any logging library that writes to stdout or stderr
. To learn more, see Log and monitor Node.js Lambda functions
. To learn about logging in other runtimes, see the 'Building with' pages for the runtimes you're interested in. Python To modify the code in the console Choose the Code tab. In the console's built-in code editor, you should see the function code that Lambda created. If you don't see the lambda_function.py tab in the code editor, select lambda_function.py in the file explorer as shown on the following diagram. Paste the following code into the lambda_function.py tab, replacing the code that Lambda created. import json
import logging logger = logging.getLogger()
logger.setLevel(logging.INFO) def lambda_handler(event, context): # Get the length and width parameters from the event object. The # runtime converts the event object to a Python dictionary length = event['length'] width = event['width'] area = calculate_area(length, width) print(f"The area is {
area}") logger.info(f"CloudWatch logs group: {
context.log_group_name}") # return the calculated area as a JSON string data = {
"area": area} return json.dumps(data) def calculate_area(length, width): return length*width In the DEPLOY section, choose Deploy to update your function's code: Understanding your function code Before you move to the next step, let's take a moment to look at the function code and understand some key Lambda concepts. The Lambda handler: Your Lambda function contains a Python function named lambda_handler
. A Lambda function in Python can contain more than one Python function, but the handler function is always the entry point to your code. When your function is invoked, Lambda runs this method. When you created your Hello world function using the console, Lambda automatically set the name of the handler method for your function to lambda_handler
. Be sure not to edit the name of this Python function. If you do, Lambda wonât be able to run your code when you invoke your function. To learn more about the Lambda handler in Python, see Define Lambda function handler in Python
. The Lambda event object: The function lambda_handler takes two arguments, event and context
. An event in Lambda is a JSON formatted document that contains data for your function to process. If your function is invoked by another AWS service, the event object contains information about the event that caused the invocation. For example, if your function is invoked when an object is uploaded to an Amazon Simple Storage Service (Amazon S3) bucket, the event contains the name of the bucket and the object key. In this example, youâll create an event in the console by entering a JSON formatted document with two key-value pairs. The Lambda context object: The second argument that your function takes is context
. Lambda passes the context object to your function automatically. The context object contains information about the function invocation and execution environment. You can use the context object to output information about your function's invocation for monitoring purposes. In this example, your function uses the log_group_name parameter to output the name of its CloudWatch log group. To learn more about the Lambda context object in Python, see Using the Lambda context object to retrieve Python function information
. Logging in Lambda: With Python, you can use either a print statement or a Python logging library to send information to your function's log. To illustrate the difference in what's captured, the example code uses both methods. In a production application, we recommend that you use a logging library. To learn more, see Log and monitor Python Lambda functions
. To learn about logging in other runtimes, see the 'Building with' pages for the runtimes you're interested in. Invoke the Lambda function using the console code editor To invoke your function using the Lambda console code editor, create a test event to send to your function. The event is a JSON formatted document containing two key-value pairs with the keys "length" and "width"
. To create the test event In the TEST EVENTS section of the console code editor, choose Create test event
. For Event Name
, enter myTestEvent
. In the Event JSON section, replace the default JSON with the following: { "length": 6, "width": 7
} Choose Save
. To test your function and view invocation records In the TEST EVENTS section of the console code editor, choose the run icon next to your test event: When your function finishes running, the response and function logs are displayed in the OUTPUT tab. You should see results similar to the following: Node.js Status: Succeeded
Test Event Name: myTestEvent Response
"
{
\"area\":42}" Function Logs
START RequestId: 5c012b0a-18f7-4805-b2f6-40912935034a Version: $LATEST
2024-08-31T23:39:45.313Z	5c012b0a-18f7-4805-b2f6-40912935034a	INFO	The area is 42
2024-08-31T23:39:45.331Z	5c012b0a-18f7-4805-b2f6-40912935034a	INFO	CloudWatch log group: /aws/lambda/myLambdaFunction
END RequestId: 5c012b0a-18f7-4805-b2f6-40912935034a
REPORT RequestId: 5c012b0a-18f7-4805-b2f6-40912935034a	Duration: 20.67 ms	Billed Duration: 21 ms	Memory Size: 128 MB	Max Memory Used: 66 MB	Init Duration: 163.87 ms Request ID
5c012b0a-18f7-4805-b2f6-40912935034a Python Status: Succeeded
Test Event Name: myTestEvent Response
"
{
\"area\": 42}" Function Logs
START RequestId: 2d0b1579-46fb-4bf7-a6e1-8e08840eae5b Version: $LATEST
The area is 42
[INFO]	2024-08-31T23:43:26.428Z	2d0b1579-46fb-4bf7-a6e1-8e08840eae5b	CloudWatch logs group: /aws/lambda/myLambdaFunction
END RequestId: 2d0b1579-46fb-4bf7-a6e1-8e08840eae5b
REPORT RequestId: 2d0b1579-46fb-4bf7-a6e1-8e08840eae5b	Duration: 1.42 ms	Billed Duration: 2 ms	Memory Size: 128 MB	Max Memory Used: 39 MB	Init Duration: 123.74 ms Request ID
2d0b1579-46fb-4bf7-a6e1-8e08840eae5b When you invoke your function outside of the Lambda console, you must use CloudWatch Logs to view your function's execution results. To view your function's invocation records in CloudWatch Logs Open the Log groups page of the CloudWatch console. Choose the log group for your function (
/aws/lambda/myLambdaFunction
). This is the log group name that your function printed to the console. Scroll down and choose the Log stream for the function invocations you want to look at. You should see output similar to the following: Node.js INIT_START Runtime Version: nodejs:22.v13 Runtime Version ARN: arn:aws:lambda:us-west-2::runtime:e3aaabf6b92ef8755eaae2f4bfdcb7eb8c4536a5e044900570a42bdba7b869d9
START RequestId: aba6c0fc-cf99-49d7-a77d-26d805dacd20 Version: $LATEST
2024-08-23T22:04:15.809Z 5c012b0a-18f7-4805-b2f6-40912935034a INFO	The area is 42
2024-08-23T22:04:15.810Z aba6c0fc-cf99-49d7-a77d-26d805dacd20 INFO CloudWatch log group: /aws/lambda/myLambdaFunction
END RequestId: aba6c0fc-cf99-49d7-a77d-26d805dacd20
REPORT RequestId: aba6c0fc-cf99-49d7-a77d-26d805dacd20 Duration: 17.77 ms Billed Duration: 18 ms Memory Size: 128 MB Max Memory Used: 67 MB Init Duration: 178.85 ms Python INIT_START Runtime Version: python:3.13.v16 Runtime Version ARN: arn:aws:lambda:us-west-2::runtime:ca202755c87b9ec2b58856efb7374b4f7b655a0ea3deb1d5acc9aee9e297b072
START RequestId: 9d4096ee-acb3-4c25-be10-8a210f0a9d8e Version: $LATEST
The area is 42
[INFO]	2024-09-01T00:05:22.464Z	9315ab6b-354a-486e-884a-2fb2972b7d84	CloudWatch logs group: /aws/lambda/myLambdaFunction
END RequestId: 9d4096ee-acb3-4c25-be10-8a210f0a9d8e REPORT RequestId: 9d4096ee-acb3-4c25-be10-8a210f0a9d8e Duration: 1.15 ms Billed Duration: 2 ms Memory Size: 128 MB Max Memory Used: 40 MB Clean up When you're finished working with the example function, delete it. You can also delete the log group that stores the function's logs, and the execution role that the console created. To delete the Lambda function Open the Functions page of the Lambda console. Select the function that you created. Choose Actions
, Delete
. Type confirm in the text input field and choose Delete
. To delete the log group Open the Log groups page of the CloudWatch console. Select the function's log group (
/aws/lambda/myLambdaFunction
). Choose Actions
, Delete log group(s)
. In the Delete log group(s) dialog box, choose Delete
. To delete the execution role Open the Roles page of the AWS Identity and Access Management (IAM) console. Select the function's execution role (for example, myLambdaFunction-role-
31exxmpl
). Choose Delete
. In the Delete role dialog box, enter the role name, and then choose Delete
. Additional resources and next steps Now that youâve created and tested a simple Lambda function using the console, take these next steps: Learn to add dependencies to your function and deploy it using a .zip deployment package. Choose your preferred language from the following links. Node.js Deploy Node.js Lambda functions with .zip file archives Typescript Deploy transpiled TypeScript code in Lambda with .zip file archives Python Working with .zip file archives for Python Lambda functions Ruby Deploy Ruby Lambda functions with .zip file archives Java Deploy Java Lambda functions with .zip or JAR file archives Go Deploy Go Lambda functions with .zip file archives C# Build and deploy C# Lambda functions with .zip file archives To learn how to invoke a Lambda function using another AWS service, see Tutorial: Using an Amazon S3 trigger to invoke a Lambda function
. Choose one of the following tutorials for more complex examples of using Lambda with other AWS services. Tutorial: Using Lambda with API Gateway
: Create an Amazon API Gateway REST API that invokes a Lambda function. Using a Lambda function to access an Amazon RDS database
: Use a Lambda function to write data to an Amazon Relational Database Service (Amazon RDS) database through RDS Proxy. Using an Amazon S3 trigger to create thumbnail images
: Use a Lambda function to create a thumbnail every time an image file is uploaded to an Amazon S3 bucket. Javascript is disabled or is unavailable in your browser.
To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.
Document Conventions
Designing an application
Example apps and patterns ============================== Understanding Lambda function invocation methods
After you deploy your Lambda function, you can invoke it in several ways: The Lambda console â Use the Lambda console to quickly create a test event to invoke your function. The AWS SDK â Use the AWS SDK to programmatically invoke your function. The Invoke API â Use the Lambda Invoke API to directly invoke your function. The AWS Command Line Interface (AWS CLI) â Use the aws lambda invoke AWS CLI command to directly invoke your function from the command line. A function URL HTTP(S) endpoint â Use function URLs to create a dedicated HTTP(S) endpoint that you can use to invoke your function. All of these methods are direct ways to invoke your function. In Lambda, a common use case is to invoke your function based on an event that occurs elsewhere in your application. Some services can invoke a Lambda function with each new event. This is called a trigger
. For stream and queue-based services, Lambda invokes the function with batches of records. This is called an event source mapping
.
When you invoke a function, you can choose to invoke it synchronously or asynchronously. With synchronous invocation
, you wait for the function to process the event and return a response. With asynchronous invocation
, Lambda queues the event for processing and returns a response immediately. The InvocationType request parameter in the Invoke API determines how Lambda invokes your function. A value of RequestResponse indicates synchronous invocation, and a value of Event indicates asynchronous invocation.
To invoke your function over IPv6, use Lambda's public dual-stack endpoints
. Dual-stack endpoints support both IPv4 and IPv6. Lambda dual-stack endpoints use the following syntax:
protocol
://lambda.
us-east-1
.api.aws
You can also use Lambda function URLs to invoke functions over IPv6. Function URL endpoints have the following format:
https://
url-id
.lambda-url.
us-east-1
.on.aws
If the function invocation results in an error, for synchronous invocations, view the error message in the response and retry the invocation manually. For asynchronous invocations, Lambda handles retries automatically and can send invocation records to a destination
. Javascript is disabled or is unavailable in your browser.
To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.
Document Conventions
Tutorial: Creating a response streaming function with a function URL
Invoke a function synchronously ============================== What is Amazon API Gateway?
Amazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST, HTTP, and WebSocket APIs at any scale. API developers can create APIs that access AWS or other web services, as well as data stored in the AWS Cloud
. As an API Gateway API developer, you can create APIs for use in your own client applications. Or you can make your APIs available to third-party app developers. For more information, see Who uses API Gateway?
.
API Gateway creates RESTful APIs that: Are HTTP-based. Enable stateless client-server communication. Implement standard HTTP methods such as GET, POST, PUT, PATCH, and DELETE. For more information about API Gateway REST APIs and HTTP APIs, see Choose between REST APIs and HTTP APIs
, API Gateway HTTP APIs
, Use API Gateway to create REST APIs
, and Develop REST APIs in API Gateway
.
API Gateway creates WebSocket APIs that: Adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server. Route incoming messages based on message content. For more information about API Gateway WebSocket APIs, see Use API Gateway to create WebSocket APIs and Overview of WebSocket APIs in API Gateway
.
Topics
Architecture of API Gateway
Features of API Gateway
API Gateway use cases
Accessing API Gateway
Part of AWS serverless infrastructure
How to get started with Amazon API Gateway
Amazon API Gateway concepts
Choose between REST APIs and HTTP APIs
Get started with the REST API console Architecture of API Gateway The following diagram shows API Gateway architecture. This diagram illustrates how the APIs you build in Amazon API Gateway provide you or your developer customers with an integrated and consistent developer experience for building AWS serverless applications. API Gateway handles all the tasks involved in accepting and processing up to hundreds of thousands of concurrent API calls. These tasks include traffic management, authorization and access control, monitoring, and API version management. API Gateway acts as a "front door" for applications to access data, business logic, or functionality from your backend services, such as workloads running on Amazon Elastic Compute Cloud (Amazon EC2), code running on AWS Lambda, any web application, or real-time communication applications. Features of API Gateway Amazon API Gateway offers features such as the following: Support for stateful (
WebSocket
) and stateless (
HTTP and REST
) APIs. Powerful, flexible authentication mechanisms, such as AWS Identity and Access Management policies, Lambda authorizer functions, and Amazon Cognito user pools. Canary release deployments for safely rolling out changes. CloudTrail logging and monitoring of API usage and API changes. CloudWatch access logging and execution logging, including the ability to set alarms. For more information, see Monitor REST API execution with Amazon CloudWatch metrics and Monitor WebSocket API execution with CloudWatch metrics
. Ability to use CloudFormation templates to enable API creation. For more information, see Amazon API Gateway Resource Types Reference and Amazon API Gateway V2 Resource Types Reference
. Support for custom domain names
. Integration with AWS WAF for protecting your APIs against common web exploits. Integration with AWS X-Ray for understanding and triaging performance latencies. For a complete list of API Gateway feature releases, see Document history
. Accessing API Gateway You can access Amazon API Gateway in the following ways: AWS Management Console â The AWS Management Console provides a web interface for creating and managing APIs. After you complete the steps in Set up to use API Gateway
, you can access the API Gateway console at https://console.aws.amazon.com/apigateway
. AWS SDKs â If you're using a programming language that AWS provides an SDK for, you can use an SDK to access API Gateway. SDKs simplify authentication, integrate easily with your development environment, and provide access to API Gateway commands. For more information, see Tools for Amazon Web Services
. API Gateway V1 and V2 APIs â If you're using a programming language that an SDK isn't available for, see the Amazon API Gateway Version 1 API Reference and Amazon API Gateway Version 2 API Reference
. AWS Command Line Interface â For more information, see Getting Set Up with the AWS Command Line Interface in the AWS Command Line Interface User Guide
. AWS Tools for Windows PowerShell â For more information, see Setting Up the AWS Tools for Windows PowerShell in the AWS Tools for PowerShell User Guide
. Part of AWS serverless infrastructure Together with AWS Lambda
, API Gateway forms the app-facing part of the AWS serverless infrastructure. To learn more about getting started with serverless, see the Serverless Developer Guide
. For an app to call publicly available AWS services, you can use Lambda to interact with required services and expose Lambda functions through API methods in API Gateway. AWS Lambda runs your code on a highly available computing infrastructure. It performs the necessary execution and administration of computing resources. To enable serverless applications, API Gateway supports streamlined proxy integrations with AWS Lambda and HTTP endpoints. How to get started with Amazon API Gateway For an introduction to Amazon API Gateway, see the following: Get started with API Gateway
, which provides a walkthrough for creating an HTTP API. Serverless land
, which provides instructional videos. Happy Little API Shorts
, which is a series of brief instructional videos. Javascript is disabled or is unavailable in your browser.
To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.
Document Conventions
API Gateway use cases ============================== Invoke REST APIs in API Gateway
To call a deployed API, clients submit requests to the URL for the API Gateway component service for API execution, known as execute-api
.
The base URL for REST APIs is in the following format: https://
api-id
.execute-api.
region
.amazonaws.com/
stage
/
where api-id is the API identifier, region is the AWS Region, and stage is the stage name of the API deployment. Important
Before you can invoke an API, you must deploy it in API Gateway. For instructions on deploying an API, see Deploy REST APIs in API Gateway
. Topics
Obtaining an API's invoke URL
Invoking an API
Use the API Gateway console to test a REST API method
Use a Java SDK generated by API Gateway for a REST API
Use an Android SDK generated by API Gateway for a REST API
Use a JavaScript SDK generated by API Gateway for a REST API
Use a Ruby SDK generated by API Gateway for a REST API
Use iOS SDK generated by API Gateway for a REST API in Objective-C or Swift Obtaining an API's invoke URL You can use the console, the AWS CLI, or an exported OpenAPI definition to obtain an API's invoke URL. Obtaining an API's invoke URL using the console The following procedure shows how to obtain an API's invoke URL in the REST API console. To obtain an API's invoke URL using the REST API console
Sign in to the API Gateway console at https://console.aws.amazon.com/apigateway
.
Choose a deployed API. From the main navigation pane, choose Stage
. Under Stage details
, choose the copy icon to copy your API's invoke URL. This URL is for the root resource of your API. To obtain an API's invoke URL for another resource in your API, expand the stage under the secondary navigation pane, and then choose a method.
Choose the copy icon to copy your API's resource-level invoke URL. Obtaining an API's invoke URL using the AWS CLI The following procedure shows how to obtain an API's invoke URL using the AWS CLI. To obtain an API's invoke URL using the AWS CLI
Use the following command to obtain the rest-api-id
. This command returns all rest-api-id values in your Region. For more information, see get-rest-apis
. aws apigateway get-rest-apis
Replace the example rest-api-id with your rest-api-id
, replace the example {
stage-name} with your {
stage-name}
, and replace the {
region}
, with your Region. https://
{
restapi_id}
.execute-api.
{
region}
.amazonaws.com/
{
stage_name}
/ Obtaining an API's invoke URL using the exported OpenAPI definition file of the API You can also construct the root URL by combining the host and basePath fields of an exported OpenAPI definition file of the API. For instructions on how to export your API, see Export a REST API from API Gateway
. Invoking an API You can call your deployed API using the browser, curl, or other applications, like Postman
. Additionally, you can use the API Gateway console to test an API call. Test uses the API Gateway's TestInvoke feature, which allows API testing before the API is deployed. For more information, see Use the API Gateway console to test a REST API method
. Note
Query string parameter values in an invocation URL cannot contain %%
. Invoking an API using a web browser If your API permits anonymous access, you can use any web browser to invoke any GET method. Enter the complete invocation URL in the browser's address bar. For other methods or any authentication-required calls, you must specify a payload or sign the requests. You can handle these in a script behind an HTML page or in a client application using one of the AWS SDKs. Invoking an API using curl You can use a tool like curl in your terminal to call your API. The following example curl command invokes the GET method on the getUsers resource of the prod stage of an API. Linux or Macintosh curl -X GET 'https://
b123abcde4
.execute-api.
us-west-2
.amazonaws.com/prod/getUsers' Windows curl -X GET "https://
b123abcde4
.execute-api.
us-west-2
.amazonaws.com/prod/getUsers" Javascript is disabled or is unavailable in your browser.
To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.
Document Conventions
Access binary files in Lambda using an API Gateway API
Use the console to test a REST API method ============================== What is IAM?
AWS Identity and Access Management (IAM) is a web service that helps you securely control access to AWS resources. With IAM, you can manage permissions that control which AWS resources users can access. You use IAM to control who is authenticated (signed in) and authorized (has permissions) to use resources. IAM provides the infrastructure necessary to control authentication and authorization for your AWS accounts. Identities When you create an AWS account, you begin with one sign-in identity called the AWS account root user that has complete access to all AWS services and resources. We strongly recommend that you don't use the root user for everyday tasks. For tasks that require root user credentials, see Tasks that require root user credentials in the IAM User Guide
. Use IAM to set up other identities in addition to your root user, such as administrators, analysts, and developers, and grant them access to the resources they need to succeed in their tasks. Access management After a user is set up in IAM, they use their sign-in credentials to authenticate with AWS. Authentication is provided by matching the sign-in credentials to a principal (an IAM user, AWS STS federated principal, IAM role, or application) trusted by the AWS account. Next, a request is made to grant the principal access to resources. Access is granted in response to an authorization request if the user has been given permission to the resource. For example, when you first sign in to the console and are on the console Home page, you aren't accessing a specific service. When you select a service, the request for authorization is sent to that service and it looks to see if your identity is on the list of authorized users, what policies are being enforced to control the level of access granted, and any other policies that might be in effect. Authorization requests can be made by principals within your AWS account or from another AWS account that you trust. Once authorized, the principal can take action or perform operations on resources in your AWS account. For example, the principal could launch a new Amazon Elastic Compute Cloud instance, modify IAM group membership, or delete Amazon Simple Storage Service buckets. Tip
AWS Training and Certification provides a 10-minute video introduction to IAM:
Introduction to AWS Identity and Access Management
. Service availability IAM, like many other AWS services, is eventually consistent
. IAM achieves high availability by replicating data across multiple servers within Amazon's data centers around the world. If a request to change some data is successful, the change is committed and safely stored. However, the change must be replicated across IAM, which can take some time. Such changes include creating or updating users, groups, roles, or policies. We recommend that you do not include such IAM changes in the critical, high-availability code paths of your application. Instead, make IAM changes in a separate initialization or setup routine that you run less frequently. Also, be sure to verify that the changes have been propagated before production workflows depend on them. For more information, see Changes that I make are not always immediately visible
. Service cost information AWS Identity and Access Management (IAM), AWS IAM Identity Center and AWS Security Token Service (AWS STS) are features of your AWS account offered at no additional charge. You are charged only when you access other AWS services using your IAM users or AWS STS temporary security credentials. IAM Access Analyzer external access analysis is offered at no additional charge. However, you will incur charges for unused access analysis and customer policy checks. For a complete list of charges and prices for IAM Access Analyzer, see IAM Access Analyzer pricing
. For information about the pricing of other AWS products, see the Amazon Web Services pricing page
. Integration with other AWS services IAM is integrated with many AWS services. For a list of AWS services that work with IAM and the IAM features the services support, see AWS services that work with IAM
. Javascript is disabled or is unavailable in your browser.
To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.
Document Conventions
Why should I use IAM? ============================== Policies and permissions in AWS Identity and Access Management
Manage access in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. A policy is an object in AWS that, when associated with an identity or resource, defines their permissions. AWS evaluates these policies when an IAM principal (user or role) makes a request. Permissions in the policies determine whether the request is allowed or denied. Most policies are stored in AWS as JSON documents. AWS supports seven types of policies: identity-based policies, resource-based policies, permissions boundaries, AWS Organizations service control policies (SCPs), AWS Organizations resource control policies (RCPs), access control lists (ACLs), and session policies.
IAM policies define permissions for an action regardless of the method that you use to perform the operation. For example, if a policy allows the GetUser action, then a user with that policy can get user information from the AWS Management Console, the AWS CLI, or the AWS API. When you create an IAM user, you can choose to allow console or programmatic access. If console access is allowed, the IAM user can sign in to the console using their sign-in credentials. If programmatic access is allowed, the user can use access keys to work with the CLI or API. Policy types The following policy types, listed in order from most frequently used to less frequently used, are available for use in AWS. For more details, see the sections below for each policy type. Identity-based policies â Attach managed and inline policies to IAM identities (users, groups to which users belong, or roles). Identity-based policies grant permissions to an identity. Resource-based policies â Attach inline policies to resources. The most common examples of resource-based policies are Amazon S3 bucket policies and IAM role trust policies. Resource-based policies grant permissions to the principal that is specified in the policy. Principals can be in the same account as the resource or in other accounts. Permissions boundaries â Use a managed policy as the permissions boundary for an IAM entity (user or role). That policy defines the maximum permissions that the identity-based policies can grant to an entity, but does not grant permissions. Permissions boundaries do not define the maximum permissions that a resource-based policy can grant to an entity. AWS Organizations SCPs â Use an AWS Organizations service control policy (SCP) to define the maximum permissions for IAM users and IAM roles within accounts in your organization or organizational unit (OU). SCPs limit permissions that identity-based policies or resource-based policies grant to IAM users or IAM roles within the account. SCPs do not grant permissions. AWS Organizations RCPs â Use an AWS Organizations resource control policy (RCP) to define the maximum permissions for resources within accounts in your organization or organizational unit (OU). RCPs limit permissions that identity-based and resource-based policies can grant to resources in accounts within your organization. RCPs do not grant permissions. Access control lists (ACLs) â Use ACLs to control which principals in other accounts can access the resource to which the ACL is attached. ACLs are similar to resource-based policies, although they are the only policy type that does not use the JSON policy document structure. ACLs are cross-account permissions policies that grant permissions to the specified principal. ACLs cannot grant permissions to entities within the same account. Session policies â Pass advanced session policies when you use the AWS CLI or AWS API to assume a role or a federated user. Session policies limit the permissions that the role or user's identity-based policies grant to the session. Session policies limit permissions for a created session, but do not grant permissions. For more information, see Session Policies
. Identity-based policies Identity-based policies are JSON permissions policy documents that control what actions an identity (users, groups of users, and roles) can perform, on which resources, and under what conditions. Identity-based policies can be further categorized: Managed policies â Standalone identity-based policies that you can attach to multiple users, groups, and roles in your AWS account. There are two types of managed policies: AWS managed policies â Managed policies that are created and managed by AWS. Customer managed policies â Managed policies that you create and manage in your AWS account. Customer managed policies provide more precise control over your policies than AWS managed policies. Inline policies â Policies that you add directly to a single user, group, or role. Inline policies maintain a strict one-to-one relationship between a policy and an identity. They are deleted when you delete the identity. To learn how to choose between managed and inline policies, see Choose between managed policies and inline policies
. Resource-based policies Resource-based policies are JSON policy documents that you attach to a resource such as an Amazon S3 bucket. These policies grant the specified principal permission to perform specific actions on that resource and defines under what conditions this applies. Resource-based policies are inline policies. There are no managed resource-based policies. To enable cross-account access, you can specify an entire account or IAM entities in another account as the principal in a resource-based policy. Adding a cross-account principal to a resource-based policy is only half of establishing the trust relationship. When the principal and the resource are in separate AWS accounts, you must also use an identity-based policy to grant the principal access to the resource. However, if a resource-based policy grants access to a principal in the same account, no additional identity-based policy is required. For step-by step instructions for granting cross-service access, see IAM tutorial: Delegate access across AWS accounts using IAM roles
. The IAM service supports only one type of resource-based policy called a role trust policy
, which is attached to an IAM role. An IAM role is both an identity and a resource that supports resource-based policies. For that reason, you must attach both a trust policy and an identity-based policy to an IAM role. Trust policies define which principal entities (accounts, users, roles, and federated users) can assume the role. To learn how IAM roles are different from other resource-based policies, see Cross account resource access in IAM
. To see which other services support resource-based policies, see AWS services that work with IAM
. To learn more about resource-based policies, see Identity-based policies and resource-based policies
. To learn whether principals in accounts outside of your zone of trust (trusted organization or account) have access to assume your roles, see What is IAM Access Analyzer?
. IAM permissions boundaries A permissions boundary is an advanced feature in which you set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by both its identity-based policies and its permissions boundaries. If you specify a role session or user in the principal element of a resource-based policy, an explicit allow in the permission boundary is not required. However, if you specify a role ARN in the principal element of a resource-based policy, an explicit allow in the permission boundary is required. In both cases, an explicit deny in the permission boundary is effective. An explicit deny in any of these policies overrides the allow. For more information about permissions boundaries, see Permissions boundaries for IAM entities
. AWS Organizations service control policies (SCPs) If you enable all features in an organization, then you can apply service control policies (SCPs) to any or all of your accounts. SCPs are JSON policies that specify the maximum permissions for IAM users and IAM roles within accounts of an organization or organizational unit (OU). The SCP limits permissions for principals in member accounts, including each AWS account root user. An explicit deny in any of these policies overrides an allow in other policies. For more information about AWS Organizations and SCPs, see Service control policies (SCPs) in the AWS Organizations User Guide
. AWS Organizations resource control policies (RCPs) If you enable all features in an organization, then you can use resource control policies (RCPs) to centrally apply access controls on resources across multiple AWS accounts. RCPs are JSON policies that you can use to set the maximum available permissions for resources in your accounts without updating the IAM policies attached to each resource that you own. The RCP limits permissions for resources in member accounts and can impact the effective permissions for identities, including the AWS account root user, regardless of whether they belong to your organization. An explicit deny in any applicable RCP overrides an allow in other policies that might be attached to individual identities or resources. For more information about AWS Organizations and RCPs including a list of AWS services that support RCPs, see Resource control policies (RCPs) in the AWS Organizations User Guide
. Access control lists (ACLs) Access control lists (ACLs) are service policies that allow you to control which principals in another account can access a resource. ACLs cannot be used to control access for a principal within the same account. ACLs are similar to resource-based policies, although they are the only policy type that does not use the JSON policy document format. Amazon S3, AWS WAF, and Amazon VPC are examples of services that support ACLs. To learn more about ACLs, see Access Control List (ACL) overview in the Amazon Simple Storage Service Developer Guide
. Session policies Session policies are advanced policies that you pass as a parameter when you programmatically create a temporary session for a role or an AWS STS federated user principal. The permissions for a session are the intersection of the identity-based policies for the IAM entity (user or role) used to create the session and the session policies. Permissions can also come from a resource-based policy. An explicit deny in any of these policies overrides the allow. You can create role session and pass session policies programmatically using the AssumeRole
, AssumeRoleWithSAML
, or AssumeRoleWithWebIdentity API operations. You can pass a single JSON inline session policy document using the Policy parameter. You can use the PolicyArns parameter to specify up to 10 managed session policies. For more information about creating a role session, see Permissions for temporary security credentials
. When you create an AWS STS federated user principal session, you use the access keys of the IAM user to programmatically call the GetFederationToken API operation. You must also pass session policies. The resulting session's permissions are the intersection of the identity-based policy and the session policy. For more information about creating a federated user session, see Requesting credentials through a custom identity broker
. A resource-based policy can specify the ARN of the user or role as a principal. In that case, the permissions from the resource-based policy are added to the role or user's identity-based policy before the session is created. The session policy limits the total permissions granted by the resource-based policy and the identity-based policy. The resulting session's permissions are the intersection of the session policies and the resource-based policies plus the intersection of the session policies and identity-based policies. A resource-based policy can specify the ARN of the session as a principal. In that case, the permissions from the resource-based policy are added after the session is created. The resource-based policy permissions are not limited by the session policy. The resulting session has all the permissions of the resource-based policy plus the intersection of the identity-based policy and the session policy. A permissions boundary can set the maximum permissions for a user or role that is used to create a session. In that case, the resulting session's permissions are the intersection of the session policy, the permissions boundary, and the identity-based policy. However, a permissions boundary does not limit permissions granted by a resource-based policy that specifies the ARN of the resulting session. Policies and the root user The AWS account root user is affected by some policy types but not others. You cannot attach identity-based policies to the root user, and you cannot set the permissions boundary for the root user. However, you can specify the root user as the principal in a resource-based policy or an ACL. A root user is still the member of an account. If that account is a member of an organization in AWS Organizations, the root user is affected by SCPs and RCPs for the account. Overview of JSON policies Most policies are stored in AWS as JSON documents. Identity-based policies and policies used to set permissions boundaries are JSON policy documents that you attach to a user or role. Resource-based policies are JSON policy documents that you attach to a resource. SCPs and RCPs are JSON policy documents with restricted syntax that you attach to the AWS Organizations' organization root, organizational unit (OU), or an account. ACLs are also attached to a resource, but you must use a different syntax. Session policies are JSON policies that you provide when you assume a role or federated user session. It is not necessary for you to understand the JSON syntax. You can use the visual editor in the AWS Management Console to create and edit customer managed policies without ever using JSON. However, if you use inline policies for groups or complex policies, you must still create and edit those policies in the JSON editor using the console. For more information about using the visual editor, see Define custom IAM permissions with customer managed policies and Edit IAM policies
. When you create or edit a JSON policy, IAM can perform policy validation to help you create an effective policy. IAM identifies JSON syntax errors, while IAM Access Analyzer provides additional policy checks with recommendations to help you further refine your policies. To learn more about policy validation, see IAM policy validation
. To learn more about IAM Access Analyzer policy checks and actionable recommendations, see IAM Access Analyzer policy validation
. JSON policy document structure As illustrated in the following figure, a JSON policy document includes these elements: Optional policy-wide information at the top of the document One or more individual statements Each statement includes information about a single permission. If a policy includes multiple statements, AWS applies a logical OR across the statements when evaluating them. If multiple policies apply to a request, AWS applies a logical OR across all of those policies when evaluating them. The information in a statement is contained within a series of elements. Version â Specify the version of the policy language that you want to use. We recommend that you use the latest 2012-10-17 version. For more information, see IAM JSON policy elements: Version Statement â Use this main policy element as a container for the following elements. You can include more than one statement in a policy. Sid (Optional) â Include an optional statement ID to differentiate between your statements. Effect â Use Allow or Deny to indicate whether the policy allows or denies access. Principal (Required in some circumstances) â If you create a resource-based policy, you must indicate the account, user, role, or AWS STS federated user principal to which you would like to allow or deny access. If you are creating an IAM permissions policy to attach to a user or role, you cannot include this element. The principal is implied as that user or role. Action â Include a list of actions that the policy allows or denies. Resource (Required in some circumstances) â If you create an IAM permissions policy, you must specify a list of resources to which the actions apply. If you create a resource-based policy, it depends on the resource you're using as to whether this element is required or not. Condition (Optional) â Specify the circumstances under which the policy grants permission. To learn about these and other more advanced policy elements, see IAM JSON policy element reference
. Multiple statements and multiple policies If you want to define more than one permission for an entity (user or role), you can use multiple statements in a single policy. You can also attach multiple policies. If you try to define multiple permissions in a single statement, your policy might not grant the access that you expect. We recommend that you break up policies by resource type. Because of the limited size of policies
, it might be necessary to use multiple policies for more complex permissions. It's also a good idea to create functional groupings of permissions in a separate customer managed policy. For example, Create one policy for IAM user management, one for self-management, and another policy for S3 bucket management. Regardless of the combination of multiple statements and multiple policies, AWS evaluates your policies the same way. For example, the following policy has three statements, each of which defines a separate set of permissions within a single account. The statements define the following: The first statement, with an Sid (Statement ID) of FirstStatement
, lets the user with the attached policy change their own password. The Resource element in this statement is "
*
" (which means "all resources"). But in practice, the ChangePassword API operation (or equivalent change-password CLI command) affects only the password for the user who makes the request. The second statement lets the user list all the Amazon S3 buckets in their AWS account. The Resource element in this statement is "*" (which means "all resources"). But because policies don't grant access to resources in other accounts, the user can list only the buckets in their own AWS account. The third statement lets the user list and retrieve any object that is in a bucket named amzn-s3-demo-bucket-confidential-data
, but only when the user is authenticated with multi-factor authentication (MFA). The Condition element in the policy enforces the MFA authentication. When a policy statement contains a Condition element, the statement is only in effect when the Condition element evaluates to true. In this case, the Condition evaluates to true when the user is MFA-authenticated. If the user is not MFA-authenticated, this Condition evaluates to false. In that case, the third statement in this policy does not apply and the user does not have access to the amzn-s3-demo-bucket-confidential-data bucket. JSON { "Version":"2012-10-17", "Statement": [ { "Sid": "FirstStatement", "Effect": "Allow", "Action": ["iam:ChangePassword"], "Resource": "*" }, { "Sid": "SecondStatement", "Effect": "Allow", "Action": "s3:ListAllMyBuckets", "Resource": "*" }, { "Sid": "ThirdStatement", "Effect": "Allow", "Action": [ "s3:List*", "s3:Get*" ], "Resource": [ "arn:aws:s3:::amzn-s3-demo-bucket-confidential-data", "arn:aws:s3:::amzn-s3-demo-bucket-confidential-data/*" ], "Condition": {
"Bool": {
"aws:MultiFactorAuthPresent": "true"}} } ]
} Examples of JSON policy syntax The following identity-based policy allows the implied principal to list a single Amazon S3 bucket named amzn-s3-demo-bucket
: JSON { "Version":"2012-10-17", "Statement": { "Effect": "Allow", "Action": "s3:ListBucket", "Resource": "arn:aws:s3:::amzn-s3-demo-bucket" }
} The following resource-based policy can be attached to an Amazon S3 bucket. The policy allows members of a specific AWS account to perform any Amazon S3 actions in the bucket named amzn-s3-demo-bucket
. It allows any action that can be performed on a bucket or the objects within it. (Because the policy grants trust only to the account, individual users in the account must still be granted permissions for the specified Amazon S3 actions.) JSON { "Version":"2012-10-17", "Statement": [ { "Sid": "1", "Effect": "Allow", "Principal": { "AWS": [ "arn:aws:iam::
111122223333
:root" ] }, "Action": "s3:*", "Resource": [ "arn:aws:s3:::amzn-s3-demo-bucket", "arn:aws:s3:::amzn-s3-demo-bucket/*" ] } ]
} To view example policies for common scenarios, see Example IAM identity-based policies
. Grant least privilege When you create IAM policies, follow the standard security advice of granting least privilege
, or granting only the permissions required to perform a task. Determine what users and roles need to do and then craft policies that allow them to perform only those tasks. Start with a minimum set of permissions and grant additional permissions as necessary. Doing so is more secure than starting with permissions that are too lenient and then trying to tighten them later. As an alternative to least privilege, you can use AWS managed policies or policies with wildcard * permissions to get started with policies. Consider the security risk of granting your principals more permissions than they need to do their job. Monitor those principals to learn which permissions they are using. Then write least privilege policies. IAM provides several options to help you refine the permissions that you grant. Understand access level groupings â You can use access level groupings to understand the level of access that a policy grants. Policy actions are classified as List
, Read
, Write
, Permissions management
, or Tagging
. For example, you can choose actions from the List and Read access levels to grant read-only access to your users. To learn how to use policy summaries to understand access level permissions, see Access levels in policy summaries
. Validate your policies â You can perform policy validation using IAM Access Analyzer when you create and edit JSON policies. We recommend that you review and validate all of your existing policies. IAM Access Analyzer provides over 100 policy checks to validate your policies. It generates security warnings when a statement in your policy allows access we consider overly permissive. You can use the actionable recommendations that are provided through the security warnings as you work toward granting least privilege. To learn more about policy checks provided by IAM Access Analyzer, see IAM Access Analyzer policy validation
. Generate a policy based on access activity â To help you refine the permissions that you grant, you can generate an IAM policy that is based on the access activity for an IAM entity (user or role). IAM Access Analyzer reviews your AWS CloudTrail logs and generates a policy template that contains the permissions that have been used by the entity in your specified time frame. You can use the template to create a managed policy with fine-grained permissions and then attach it to the IAM entity. That way, you grant only the permissions that the user or role needs to interact with AWS resources for your specific use case. To learn more, see IAM Access Analyzer policy generation
. Use last accessed information â Another feature that can help with least privilege is last accessed information
. View this information on the Access Advisor tab on the IAM console details page for an IAM user, group, role, or policy. Last accessed information also includes information about the actions that were last accessed for some services, such as Amazon EC2, IAM, Lambda, and Amazon S3. If you sign in using AWS Organizations management account credentials, you can view service last accessed information in the AWS Organizations section of the IAM console. You can also use the AWS CLI or AWS API to retrieve a report for last accessed information for entities or policies in IAM or AWS Organizations. You can use this information to identify unnecessary permissions so that you can refine your IAM or AWS Organizations policies to better adhere to the principle of least privilege. For more information, see Refine permissions in AWS using last accessed information
. Review account events in AWS CloudTrail â To further reduce permissions, you can view your account's events in AWS CloudTrail Event history
. CloudTrail event logs include detailed event information that you can use to reduce the policy's permissions. The logs include only the actions and resources that your IAM entities need. For more information, see Viewing CloudTrail Events in the CloudTrail Console in the AWS CloudTrail User Guide
. For more information, see the following policy topics for individual services, which provide examples of how to write policies for service-specific resources. Using resource-based policies for DynamoDB in the Amazon DynamoDB Developer Guide Bucket policies for Amazon S3 in the Amazon Simple Storage Service User Guide Access Control List (ACL) overview in the Amazon Simple Storage Service User Guide Javascript is disabled or is unavailable in your browser.
To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.
Document Conventions
Access management
Managed policies and inline policies ============================== Security best practices in IAM
To help secure your AWS resources, follow these best practices for AWS Identity and Access Management (IAM).
Topics
Require human users to use federation with an identity provider to access AWS using temporary credentials
Require workloads to use temporary credentials with IAM roles to access AWS
Require multi-factor authentication (MFA)
Update access keys when needed for use cases that require long-term credentials
Follow best practices to protect your root user credentials
Apply least-privilege permissions
Get started with AWS managed policies and move toward least-privilege permissions
Use IAM Access Analyzer to generate least-privilege policies based on access activity
Regularly review and remove unused users, roles, permissions, policies, and credentials
Use conditions in IAM policies to further restrict access
Verify public and cross-account access to resources with IAM Access Analyzer
Use IAM Access Analyzer to validate your IAM policies to ensure secure and functional permissions
Establish permissions guardrails across multiple accounts
Use permissions boundaries to delegate permissions management within an account Require human users to use federation with an identity provider to access AWS using temporary credentials Human users, also known as human identities, are the people, administrators, developers, operators, and consumers of your applications. They must have an identity to access your AWS environments and applications. Human users that are members of your organization are also known as workforce identities. Human users can also be external users with whom you collaborate, and who interact with your AWS resources. They can do this via a web browser, client application, mobile app, or interactive command-line tools. Require your human users to use temporary credentials when accessing AWS. You can use an identity provider for your human users to provide federated access to AWS accounts by assuming roles, which provide temporary credentials. For centralized access management, we recommend that you use AWS IAM Identity Center (IAM Identity Center) to manage access to your accounts and permissions within those accounts. You can manage your user identities with IAM Identity Center, or manage access permissions for user identities in IAM Identity Center from an external identity provider. For more information, see What is AWS IAM Identity Center in the AWS IAM Identity Center User Guide
. For more information about roles, see Roles terms and concepts
. Require workloads to use temporary credentials with IAM roles to access AWS A workload is a collection of resources and code that delivers business value, such as an application or backend process. Your workload can have applications, operational tools, and components that require credentials to make requests to AWS services, such as requests to read data from Amazon S3. When you're building on an AWS compute service, such as Amazon EC2 or Lambda, AWS delivers the temporary credentials of an IAM role to that compute resource. Applications written using an AWS SDK will discover and use these temporary credentials to access AWS resources, and there is no need to distribute long lived credentials for an IAM user to your workloads running on AWS. Workloads that run on outside of AWS, such as your on-premises servers, servers from other cloud providers, or managed continuous integration and continuous delivery (CI/CD) platforms, can still use temporary credentials. However, you'll need to deliver these temporary credentials to your workload. The following are ways you can deliver temporary credentials to your workloads: You can use IAM Roles Anywhere to request temporary AWS credentials for your workload using an X.509 Certificate from your public key infrastructure (PKI). You can call the AWS AWS STS
AssumeRoleWithSAML API to request temporary AWS credentials for your workload using a SAML assertion from an external identity provider (IdP) that is configured within your AWS account. You can call the AWS AWS STS AssumeRoleWithWebIdentity API to request temporary AWS credentials for your workload using a JSON web token (JWT) from an IdP that is configured within your AWS account. You can request temporary AWS credentials from your IoT device using Mutual Transport Layer Security (MTLS) authentication using AWS IoT Core. Some AWS services also support integrations to deliver temporary credentials to your workloads running outside of AWS: Amazon Elastic Container Service (Amazon ECS) Anywhere lets you run Amazon ECS tasks on your own compute resources, and delivers temporary AWS credentials to your Amazon ECS tasks running on those compute resources. Amazon Elastic Kubernetes Service Hybrid Nodes lets you join your compute resources running outside of AWS as nodes to an Amazon EKS cluster. Amazon EKS can deliver temporary credentials to the Amazon EKS pods running on your compute resources. AWS Systems ManagerHybrid Activations lets you manage your compute resources running outside of AWS using SSM, and delivers temporary AWS credentials to the SSM agent running on your compute resources. Require multi-factor authentication (MFA) We recommend using IAM roles for human users and workloads that access your AWS resources so that they use temporary credentials. However, for scenarios in which you need an IAM user or root user in your account, require MFA for additional security. With MFA, users have a device that generates a response to an authentication challenge. Each user's credentials and device-generated response are required to complete the sign-in process. For more information, see AWS Multi-factor authentication in IAM
. If you use IAM Identity Center for centralized access management for human users, you can use the IAM Identity Center MFA capabilities when your identity source is configured with the IAM Identity Center identity store, AWS Managed Microsoft AD, or AD Connector. For more information about MFA in IAM Identity Center see Multi-factor authentication in the AWS IAM Identity Center User Guide
. Update access keys when needed for use cases that require long-term credentials Where possible, we recommend relying on temporary credentials instead of creating long-term credentials such as access keys. However, for scenarios in which you need IAM users with programmatic access and long-term credentials, we recommend that you update the access keys when needed, such as when an employee leaves your company. We recommend that you use IAM access last used information to update and remove access keys safely. For more information, see Update access keys
. There are specific use cases that require long-term credentials with IAM users in AWS. Some of the use cases include the following: Workloads that cannot use IAM roles â You might run a workload from a location that needs to access AWS. In some situations, you can't use IAM roles to provide temporary credentials, such as for WordPress plugins. In these situations, use IAM user long-term access keys for that workload to authenticate to AWS. Third-party AWS clients â If you are using tools that donât support access with IAM Identity Center, such as third-party AWS clients or vendors that are not hosted on AWS, use IAM user long-term access keys. AWS CodeCommit access â If you are using CodeCommit to store your code, you can use an IAM user with either SSH keys or service-specific credentials for CodeCommit to authenticate to your repositories. We recommend that you do this in addition to using a user in IAM Identity Center for normal authentication. Users in IAM Identity Center are the people in your workforce who need access to your AWS accounts or to your cloud applications. To give users access to your CodeCommit repositories without configuring IAM users, you can configure the git-remote-codecommit utility. For more information about IAM and CodeCommit, see IAM credentials for CodeCommit: Git credentials, SSH keys, and AWS access keys
. For more information about configuring the git-remote-codecommit utility, see Connecting to AWS CodeCommit repositories with rotating credentials in the AWS CodeCommit User Guide
. Amazon Keyspaces (for Apache Cassandra) access â In a situation where you are unable to use users in IAM Identity Center, such as for testing purposes for Cassandra compatibility, you can use an IAM user with service-specific credentials to authenticate with Amazon Keyspaces. Users in IAM Identity Center are the people in your workforce who need access to your AWS accounts or to your cloud applications. You can also connect to Amazon Keyspaces using temporary credentials. For more information, see Using temporary credentials to connect to Amazon Keyspaces using an IAM role and the SigV4 plugin in the Amazon Keyspaces (for Apache Cassandra) Developer Guide
. Follow best practices to protect your root user credentials When you create an AWS account, you establish root user credentials to sign in to the AWS Management Console. Safeguard your root user credentials the same way you would protect other sensitive personal information. To better understand how to secure and scale your root user processes, see Root user best practices for your AWS account
. Apply least-privilege permissions When you set permissions with IAM policies, grant only the permissions required to perform a task. You do this by defining the actions that can be taken on specific resources under specific conditions, also known as least-privilege permissions
. You might start with broad permissions while you explore the permissions that are required for your workload or use case. As your use case matures, you can work to reduce the permissions that you grant to work toward least privilege. For more information about using IAM to apply permissions, see Policies and permissions in AWS Identity and Access Management
. Get started with AWS managed policies and move toward least-privilege permissions To get started granting permissions to your users and workloads, use the AWS managed policies that grant permissions for many common use cases. They are available in your AWS account. Keep in mind that AWS managed policies might not grant least-privilege permissions for your specific use cases because they are available for use by all AWS customers. As a result, we recommend that you reduce permissions further by defining customer managed policies that are specific to your use cases. For more information, see AWS managed policies
. For more information about AWS managed policies that are designed for specific job functions, see AWS managed policies for job functions
. Use IAM Access Analyzer to generate least-privilege policies based on access activity To grant only the permissions required to perform a task, you can generate policies based on your access activity that is logged in AWS CloudTrail. IAM Access Analyzer analyzes the services and actions that your IAM roles use, and then generates a fine-grained policy that you can use. After you test each generated policy, you can deploy the policy to your production environment. This ensures that you grant only the required permissions to your workloads. For more information about policy generation, see IAM Access Analyzer policy generation
. Regularly review and remove unused users, roles, permissions, policies, and credentials You might have IAM users, roles, permissions, policies, or credentials that you no longer need in your AWS account. IAM provides last accessed information to help you identify the users, roles, permissions, policies, and credentials that you no longer need so that you can remove them. This helps you reduce the number of users, roles, permissions, policies, and credentials that you have to monitor. You can also use this information to refine your IAM policies to better adhere to least-privilege permissions. For more information, see Refine permissions in AWS using last accessed information
. Use conditions in IAM policies to further restrict access You can specify conditions under which a policy statement is in effect. That way, you can grant access to actions and resources, but only if the access request meets specific conditions. For example, you can write a policy condition to specify that all requests must be sent using TLS. You can also use conditions to grant access to service actions, but only if they are used through a specific AWS service, such as CloudFormation. For more information, see IAM JSON policy elements: Condition
. Verify public and cross-account access to resources with IAM Access Analyzer Before you grant permissions for public or cross-account access in AWS, we recommend that you verify if such access is required. You can use IAM Access Analyzer to help you preview and analyze public and cross-account access for supported resource types. You do this by reviewing the findings that IAM Access Analyzer generates. These findings help you verify that your resource access controls grant the access that you expect. Additionally, as you update public and cross-account permissions, you can verify the effect of your changes before deploying new access controls to your resources. IAM Access Analyzer also monitors supported resource types continuously and generates a finding for resources that allow public or cross-account access. For more information, see Previewing access with IAM Access Analyzer APIs
. Use IAM Access Analyzer to validate your IAM policies to ensure secure and functional permissions Validate the policies you create to ensure that they adhere to the IAM policy language (JSON) and IAM best practices. You can validate your policies by using IAM Access Analyzer policy validation. IAM Access Analyzer provides more than 100 policy checks and actionable recommendations to help you author secure and functional policies. As you author new policies or edit existing policies in the console, IAM Access Analyzer provides recommendations to help you refine and validate your policies before you save them. Additionally, we recommend that you review and validate all of your existing policies. For more information, see IAM Access Analyzer policy validation
. For more information about policy checks provided by IAM Access Analyzer, see IAM Access Analyzer policy check reference
. Establish permissions guardrails across multiple accounts As you scale your workloads, separate them by using multiple accounts that are managed with AWS Organizations. We recommend that you use AWS Organizations service control policies (SCPs) to establish permissions guardrails to control access for all principals (IAM roles and users) across your accounts. We recommend that you use AWS Organizations resource control policies (RCPs) to establish permissions guardrails to control access for AWS resources across your organization. SCPs and RCPs are types of organization policies that you can use to manage permissions in your organization at the AWS organization, organizational unit (OU), or account level. However, SCPs and RCPs alone are insufficient to grant permissions to principals and resources in your organization. No permissions are granted by SCPs and RCPs. To grant permissions, you must attach identity-based or resource-based policies to IAM users, IAM roles, or the resources in your accounts. For more information, see SRA building blocks â AWS Organizations, accounts, and guardrails
. Use permissions boundaries to delegate permissions management within an account In some scenarios, you might want to delegate permissions management within an account to others. For example, you could allow developers to create and manage roles for their workloads. When you delegate permissions to others, use permissions boundaries to set the maximum permissions that you delegate. A permissions boundary is an advanced feature for using a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM role. A permissions boundary does not grant permissions on its own. For more information, see Permissions boundaries for IAM entities
. Javascript is disabled or is unavailable in your browser.
To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.
Document Conventions
Security best practices and use cases
Root user best practices